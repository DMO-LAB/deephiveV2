{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from policies.mappo import MAPPO\n",
    "from environment.optimization_environment import OptimizationEnv\n",
    "from environment.utils import parse_config\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import neptune\n",
    "from neptune.types import File\n",
    "import argparse \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from other_algorithms.pso import ParticleSwarmOptimizer\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(config_path, mode=\"train\", **kwargs):\n",
    "    env = OptimizationEnv(config_path)\n",
    "    agent_policy = MAPPO(config_path)\n",
    "    if mode == \"test\" or mode == \"benchmark\":\n",
    "        model_path = kwargs.get(\"model_path\", None)\n",
    "        if model_path is None:\n",
    "            raise ValueError(\"Model path must be provided for testing\")\n",
    "        agent_policy.load(model_path)\n",
    "    return env, agent_policy\n",
    "\n",
    "def print_items(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key, value)\n",
    "        \n",
    "def get_action(observation_info, agent_policy, env):\n",
    "    observation, observation_std = observation_info\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    for dim in range(env.n_dim):\n",
    "        observation[dim] = observation[dim].astype(np.float32)\n",
    "        #print(observation[dim])\n",
    "        observation_std[dim] = observation_std[dim].astype(np.float32)\n",
    "        action = agent_policy.select_action(observation[dim], observation_std[dim])\n",
    "        actions[:, dim] = action\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config/config.json'\n",
    "model_path = \"models/gbest_exploiters.pth\"\n",
    "mode = \"train\"\n",
    "env, agent_policy = initialize(config_path, mode=mode, model_path=model_path)\n",
    "config = parse_config(config_path)\n",
    "agent_policy.set_action_std(config[\"test_action_std\"])\n",
    "\n",
    "def display_gmm_details(gmm):\n",
    "    print(\"Means: \", gmm.means_)\n",
    "    print(\"Covariances: \", gmm.covariances_)\n",
    "    print(\"Weights: \", gmm.weights_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_info = env.reset()\n",
    "env.render()\n",
    "env.render(type=\"gmm\")\n",
    "env.render(type=\"surrogate\")\n",
    "display_gmm_details(env.gmm.gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.get_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.evaluate_accuracy(env.objective_function.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_pos = env._get_actual_state()[:, :-1]\n",
    "agent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = env.gmm.sample_candidate_points(5)\n",
    "novelty_scores = env.gmm.assess_novelty(sample_points)\n",
    "variance_scores = [env.gmm.get_variance(point) for point in sample_points]\n",
    "# normalize the novelty scores to 0-1\n",
    "novelty_scores_norm = (novelty_scores - np.min(novelty_scores)) / (np.max(novelty_scores) - np.min(novelty_scores))\n",
    "for point, novelty_score, novelty_score_norm, variance in zip(sample_points, novelty_scores, novelty_scores_norm, variance_scores):\n",
    "    print(\"Point: \", point, \"Novelty Score: \", novelty_score, \"Novelty Score Norm: \", novelty_score_norm, \"Variance: \", variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gmm.gmm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gmm.assess_novelty(np.array([[ 0.41, -0.43]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = get_action(observation_info, agent_policy, env) \n",
    "# add noise to actions\n",
    "actions += np.random.normal(0, 0.1, actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.surrogate_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_info, reward, done, info = env.step(actions)\n",
    "env.render()\n",
    "env.render(type=\"gmm\")\n",
    "env.render(type=\"surrogate\")\n",
    "display_gmm_details(env.gmm.gmm)\n",
    "print(env.surrogate_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = env.gmm.sample_candidate_points(5)\n",
    "novelty_scores = env.gmm.assess_novelty(sample_points)\n",
    "# normalize the novelty scores to 0-1\n",
    "novelty_scores_norm = (novelty_scores - np.min(novelty_scores)) / (np.max(novelty_scores) - np.min(novelty_scores))\n",
    "for point, novelty_score, novelty_score_norm in zip(sample_points, novelty_scores, novelty_scores_norm):\n",
    "    print(\"Point: \", point, \"Novelty Score: \", novelty_score, \"Novelty Score Norm: \", novelty_score_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters  = 25\n",
    "obs = env.reset()\n",
    "for i in range(iters):\n",
    "    actions = get_action(obs, agent_policy, env)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "    if i % 5 == 0:\n",
    "        env.render(\"surrogate\")\n",
    "        print(env.gbest)\n",
    "        env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbest_hist_path = \"testing_runs/2023-11-22_17-33-45/gbest_history_8.npy\"\n",
    "gbh = np.load(gbest_hist_path)\n",
    "print(gbh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gbest_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_of_function_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "        a = 1.0 * np.array(data)\n",
    "        n = len(a)\n",
    "        m, se = np.mean(a, axis = 0), scipy.stats.sem(a)\n",
    "        h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "        return m, m-h, m+h\n",
    "\n",
    "def num_function_evaluation(fopt, n_agents, save_dir, opt_value, label=\"TEST OPT\"):\n",
    "    # convert fopt to numpy array if it is not already\n",
    "        fopt = np.array(fopt)\n",
    "        mf1 = np.mean(fopt, axis = 0)\n",
    "        err = np.std(fopt, axis = 0)\n",
    "        mf1, ml1, mh1 = mean_confidence_interval(fopt,0.95)\n",
    "\n",
    "        fig = plt.figure(figsize=(6,4), dpi=200)\n",
    "        plt.rcParams[\"figure.figsize\"] = [6, 4]\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "        plt.fill_between((np.arange(len(mf1))+1)*n_agents, ml1, mh1, alpha=0.1, edgecolor='#3F7F4C', facecolor='#7EFF99')\n",
    "        plt.plot((np.arange(len(mf1))+1)*n_agents, mf1, linewidth=2.0, label = label, color='#3F7F4C')\n",
    "        if opt_value is not None:\n",
    "            plt.plot((np.arange(len(mf1))+1)*n_agents, np.ones(len(mf1))*opt_value, linewidth=1.0, label = 'True OPT', color='#CC4F1B')\n",
    "\n",
    "        plt.xlabel('number of function evaluations', fontsize = 14)\n",
    "        plt.ylabel('best fitness value', fontsize = 14)\n",
    "\n",
    "        plt.legend(fontsize = 14, frameon=False)\n",
    "        plt.xscale('log')\n",
    "        plt.yticks(fontsize = 14)\n",
    "        plt.savefig(save_dir)\n",
    "        # close the figure\n",
    "        plt.close(fig)\n",
    "\n",
    "def plot_num_function_evaluation(fopt, n_agents, save_dir, opt_value, show_std=False, symbol_list=None, color_list=None, label_list=None, show=True, title=None):\n",
    "        # The method implementation goes here\n",
    "        fig = plt.figure(figsize=(6, 4), dpi=200)\n",
    "        plt.rcParams[\"figure.figsize\"] = [6, 4]\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        if symbol_list is None:\n",
    "            symbol_list = ['-']\n",
    "        if color_list is None:\n",
    "            color_list = ['#3F7F4C']\n",
    "        if label_list is None:\n",
    "            label_list = ['DeepHive']\n",
    "\n",
    "        print(f\"Number of function evaluations: {len(fopt[0])}\")\n",
    "        print(f\"Number of algorithms: {len(fopt)}\")\n",
    "\n",
    "        if len(fopt) == 1:\n",
    "            print(\"Single algorithm\")\n",
    "            num_function_evaluation(fopt[0], n_agents, save_dir, opt_value, label=label_list[0])\n",
    "        else:\n",
    "            for i in range(len(fopt)):\n",
    "                \n",
    "                mf1, ml1, mh1 = mean_confidence_interval(fopt[i], 0.95)\n",
    "                if show_std:\n",
    "                    plt.errorbar((np.arange(len(mf1)) + 1) * n_agents, mf1, yerr=mh1 - ml1, linewidth=2.0,\n",
    "                                label=label_list[i],\n",
    "                                color=color_list[i])\n",
    "                # plt.fill_between((np.arange(len(mf1)) + 1) * n_agents, ml1, mh1, alpha=0.1, edgecolor='#3F7F4C',\n",
    "                #                  facecolor=color_list[i])\n",
    "                plt.plot((np.arange(len(mf1)) + 1) * n_agents, mf1, symbol_list[i], linewidth=1, label=label_list[i],\n",
    "                        color=color_list[i])\n",
    "\n",
    "        if opt_value is not None:\n",
    "            plt.plot((np.arange(len(mf1))+1)*n_agents, np.ones(len(mf1))*opt_value, linewidth=0.5, label = 'True OPT', color='#CC4F1B')\n",
    "\n",
    "        plt.xlabel('number of function evaluations', fontsize=14)\n",
    "        plt.ylabel('best fitness value', fontsize=14)\n",
    "        plt.legend(fontsize=8, frameon=False, loc=\"lower right\")\n",
    "        plt.xscale('log')\n",
    "        plt.yticks(fontsize=14)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        if show:\n",
    "            plt.show()\n",
    "    \n",
    "        plt.savefig(save_dir)\n",
    "        #plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1_dir = \"benchmarking_runs/2023-11-13_10-31-46\"  # std = 0.05\n",
    "opt2_dir = \"benchmarking_runs/2023-11-13_10-34-03\" # std = 0.02\n",
    "opt3_dir = \"benchmarking_runs/2023-11-13_10-35-39\" # std = 0.035\n",
    "opt4_dir = \"benchmarking_runs/2023-11-13_10-37-39\" # learn_std - 0.3 - 0.03\n",
    "opt5_dir = \"benchmarking_runs/2023-11-13_10-39-21\" # learn_std - 0.5 - 0.005\n",
    "opt6_dir = \"benchmarking_runs/2023-11-13_10-40-59\" # unfreeze learn_std - 0.4 - 0.02\n",
    "\n",
    "opt_dirs = [opt1_dir, opt2_dir, opt3_dir, opt4_dir, opt5_dir, opt6_dir]\n",
    "labels = [\"std-0.05\", \"std-0.02\", \"std-0.035\", \"ls-0.3-0.03\", \"ls-0.5-0.005\", \"unfreeze-ls-0.4-0.02\"]\n",
    "colors = [\"red\", \"green\", \"blue\", \"orange\", \"purple\", \"brown\"]\n",
    "symbols = [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "def prepare_opt_values(dir):\n",
    "    # get all the .npy files in the directory\n",
    "    opt_values = []\n",
    "    files = os.listdir(dir)\n",
    "    files = [file for file in files if file.endswith(\".npy\")]\n",
    "    for file in files:\n",
    "        opt_value = np.load(os.path.join(dir, file))\n",
    "        # grab all the columns except the last one\n",
    "        opt_value = opt_value[:, -1]\n",
    "        #print(opt_value)\n",
    "        opt_values.append(opt_value.tolist())\n",
    "    return opt_values\n",
    "\n",
    "ploting_values = []\n",
    "for dir in opt_dirs:\n",
    "    opt_values = prepare_opt_values(dir)\n",
    "    ploting_values.append(opt_values)\n",
    "\n",
    "plot_num_function_evaluation(ploting_values, 10, \"benchmarking_runs/plot.png\", 0.2, label_list=labels, show_std=False, color_list=colors, symbol_list=symbols, show=False, title=\"2D cosine mixture function benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1_dir = \"benchmarking_runs/2023-11-13_10-57-38\"  # std = 0.05\n",
    "opt2_dir = \"benchmarking_runs/2023-11-13_10-56-41\" # std = 0.02\n",
    "opt3_dir = \"benchmarking_runs/2023-11-13_10-55-41\" # std = 0.035\n",
    "opt4_dir = \"benchmarking_runs/2023-11-13_10-54-53\" # learn_std - 0.3 - 0.03\n",
    "opt5_dir = \"benchmarking_runs/2023-11-13_10-53-53\" # learn_std - 0.5 - 0.005\n",
    "opt6_dir = \"benchmarking_runs/2023-11-13_10-52-32\" # unfreeze learn_std - 0.5 - 0.05\n",
    "\n",
    "opt_dirs = [opt1_dir, opt2_dir, opt3_dir, opt4_dir, opt5_dir, opt6_dir]\n",
    "labels = [\"std-0.05\", \"std-0.02\", \"std-0.035\", \"ls-0.3-0.03\", \"ls-0.5-0.005\", \"unfreeze-ls-0.4-0.02\"]\n",
    "colors = [\"red\", \"green\", \"blue\", \"orange\", \"purple\", \"brown\"]\n",
    "symbols = [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "def prepare_opt_values(dir):\n",
    "    # get all the .npy files in the directory\n",
    "    opt_values = []\n",
    "    files = os.listdir(dir)\n",
    "    files = [file for file in files if file.endswith(\".npy\")]\n",
    "    for file in files:\n",
    "        opt_value = np.load(os.path.join(dir, file))\n",
    "        # grab all the columns except the last one\n",
    "        opt_value = opt_value[:, -1]\n",
    "        #print(opt_value)\n",
    "        opt_values.append(opt_value.tolist())\n",
    "    return opt_values\n",
    "\n",
    "ploting_values = []\n",
    "for dir in opt_dirs:\n",
    "    opt_values = prepare_opt_values(dir)\n",
    "    ploting_values.append(opt_values)\n",
    "\n",
    "plot_num_function_evaluation(ploting_values, 10, \"benchmarking_runs/plot1.png\", 0.2, label_list=labels, show_std=False, color_list=colors, symbol_list=symbols, show=False, title=\"3D cosine mixture function benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_function_evaluation([opt_values_1, opt_values_2], 1, \"test.png\", 0.2, show_std=False, symbol_list=['-', '--'], color_list=['#3F7F4C', '#CC4F1B'], label_list=['DeepHive', 'PSO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "# Define a simple 2D grid space for demonstration\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "X_flat = X.ravel()\n",
    "Y_flat = Y.ravel()\n",
    "grid_points = np.vstack((X_flat, Y_flat)).T\n",
    "\n",
    "# Generate some random sample data for GMM fitting\n",
    "n_samples = 300\n",
    "sample_data = np.random.uniform(low=-3, high=3, size=(n_samples, 2))\n",
    "\n",
    "# Fit a Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=5, random_state=42)\n",
    "gmm.fit(sample_data)\n",
    "\n",
    "# Predict the density for the grid points\n",
    "gmm_density = gmm.score_samples(grid_points)\n",
    "\n",
    "# Identify low-density regions\n",
    "low_density_threshold = np.percentile(gmm_density, 10)  # for example, the bottom 10% of densities\n",
    "low_density_points = grid_points[gmm_density < low_density_threshold]\n",
    "\n",
    "# Gaussian Process - Assume we have some function observations to fit the GP\n",
    "# For the purpose of this demonstration, we'll use the sample_data as observations\n",
    "observations = np.random.rand(n_samples)  # Placeholder for actual function observations\n",
    "\n",
    "# Define and fit Gaussian Process Regressor\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "gp.fit(sample_data, observations)\n",
    "\n",
    "# Make predictions along our grid, include the mean and the variance (uncertainty)\n",
    "gp_mean, gp_var = gp.predict(grid_points, return_std=True)\n",
    "gp_var = gp_var ** 2  # Get variance from standard deviation\n",
    "\n",
    "# Identify high-variance regions\n",
    "high_variance_threshold = np.percentile(gp_var, 90)  # for example, the top 10% of variances\n",
    "high_variance_points = grid_points[gp_var > high_variance_threshold]\n",
    "\n",
    "# Return the low-density points from GMM and high-variance points from GP\n",
    "low_density_points, high_variance_points\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eloEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
