{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from policies.mappo import MAPPO\n",
    "from environment.optimization_environment import OptimizationEnv\n",
    "from environment.utils import parse_config\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from policies.mappo import MAPPO\n",
    "from environment.optimization_environment import OptimizationEnv\n",
    "from environment.utils import parse_config\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def initialize(config_path):\n",
    "    env = OptimizationEnv(config_path)\n",
    "    agent = MAPPO(config_path)\n",
    "    return env, agent\n",
    "\n",
    "def print_items(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key, value)\n",
    "        \n",
    "def get_action(observation_info, agent, env):\n",
    "    observation, observation_std = observation_info\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    for dim in range(env.n_dim):\n",
    "        observation[dim] = observation[dim].astype(np.float32)\n",
    "        observation_std[dim] = observation_std[dim].astype(np.float32)\n",
    "        action = agent.select_action(observation[dim], observation_std[dim])\n",
    "        actions[:, dim] = action\n",
    "    return actions\n",
    "\n",
    "\n",
    "def save_episode(env, df, episode, step, observation, state, action, reward, file_path='data/'):\n",
    "    actual_state = env._get_actual_state()\n",
    "    for agent in range(env.n_agents):\n",
    "        df.loc[agent + step * env.n_agents] = {\n",
    "            'step': step,\n",
    "            'agent': agent,\n",
    "            'scaled_dim1': state[agent][0],\n",
    "            'scaled_dim2': state[agent][1],\n",
    "            'scaled_value': state[agent][2],\n",
    "            'real_dim1': actual_state[agent][0],\n",
    "            'real_dim2': actual_state[agent][1],\n",
    "            'real_value': actual_state[agent][2],\n",
    "            'action': action[agent],\n",
    "            'reward': reward[agent],\n",
    "            'obs_1_value': observation[0][agent][0],\n",
    "            #'obs_2_prev_diff_dim_1': observation[0][agent][1],\n",
    "            'obs_3_prev_diff_value': observation[0][agent][1],\n",
    "            'obs_4_pbest_diff_dim_1': observation[0][agent][2],\n",
    "            'obs_5_pbest_diff_value': observation[0][agent][3],\n",
    "            'obs_6_neigb_best_diff_dim_1': observation[0][agent][4],\n",
    "            'obs_7_neigb_best_diff_value': observation[0][agent][5],\n",
    "            'obs_8_random_neigh_diff_dim_1': observation[0][agent][6],\n",
    "            'obs_9_random_neigh_diff_value': observation[0][agent][7],\n",
    "            #'obs_10_prev_diff_dim_2': observation[1][agent][1],\n",
    "            'obs_11_pbset_diff_dim_2': observation[1][agent][2],\n",
    "            'obs_12_neigb_best_diff_dim_2': observation[1][agent][4],\n",
    "            'obs_13_random_neigh_diff_dim_2': observation[1][agent][6],\n",
    "        }\n",
    "    if step == env.ep_length - 1:\n",
    "        # sort by step \n",
    "        df = df.sort_values(by=['agent', 'step'])\n",
    "        df.to_csv('{}{}.csv'.format(file_path, episode), index=False)\n",
    "        df = pd.DataFrame(columns=['step', 'agent', 'scaled_dim1', 'scaled_dim2', 'scaled_value',\n",
    "                                    'real_dim1', 'real_dim2', 'real_value', 'action', 'reward', 'obs_1_value',\n",
    "                                      'obs_3_prev_diff_value', 'obs_4_pbest_diff_dim_1',\n",
    "                                        'obs_5_pbest_diff_value', 'obs_6_neigb_best_diff_dim_1', 'obs_7_neigb_best_diff_value',\n",
    "                                          'obs_8_random_neigh_diff_dim_1', 'obs_9_random_neigh_diff_value',\n",
    "                                           'obs_11_pbset_diff_dim_2', 'obs_12_neigb_best_diff_dim_2',\n",
    "                                              'obs_13_random_neigh_diff_dim_2'])\n",
    "    #print('episode: {}, step: {}, length: {}'.format(episode, step, len(df)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_agent(env, agent, n_episodes=2000, update_timestep=25, decay_rate=0.01, log_interval=200, min_action_std=0.1,\n",
    "                decay_interval=1000, save_interval=2000):\n",
    "    df = pd.DataFrame(columns=['step', 'agent', 'scaled_dim1', 'scaled_dim2', 'scaled_value',\n",
    "                                    'real_dim1', 'real_dim2', 'real_value', 'action', 'reward', 'obs_1_value',\n",
    "                                      'obs_3_prev_diff_value', 'obs_4_pbest_diff_dim_1',\n",
    "                                        'obs_5_pbest_diff_value', 'obs_6_neigb_best_diff_dim_1', 'obs_7_neigb_best_diff_value',\n",
    "                                          'obs_8_random_neigh_diff_dim_1', 'obs_9_random_neigh_diff_value',\n",
    "                                             'obs_11_pbset_diff_dim_2', 'obs_12_neigb_best_diff_dim_2',\n",
    "                                              'obs_13_random_neigh_diff_dim_2'])\n",
    "    average_returns = []\n",
    "    training_run_title = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    save_path = f\"training_runs/{training_run_title}/\"\n",
    "    os.mkdir(save_path)\n",
    "    timesteps = 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        observation_info = env.reset()\n",
    "        episode_return = np.zeros(env.n_agents)\n",
    "        for step in range(env.ep_length):\n",
    "            actions = get_action(observation_info, agent, env)\n",
    "            observation_info, reward, done, info = env.step(actions)\n",
    "            # add reward to agent buffer\n",
    "            for ag in range(env.n_agents):\n",
    "                agent.buffer.rewards += [reward[ag]] * env.n_dim\n",
    "                agent.buffer.is_terminals += [done[ag]] * env.n_dim\n",
    "            \n",
    "            episode_return += reward\n",
    "            if episode % log_interval == 0:\n",
    "                df = save_episode(env, df, episode, step, observation_info[0], env.prev_state, actions, reward, file_path=save_path)\n",
    "\n",
    "\n",
    "            if step == env.ep_length - 1:\n",
    "                average_returns.append(np.mean(episode_return))\n",
    "                running_average_rewards = np.mean(average_returns)\n",
    "                \n",
    "            timesteps += 1\n",
    "        if timesteps % update_timestep == 0:\n",
    "            agent.update()\n",
    "    \n",
    "        if episode % log_interval == 0 and timesteps > 0:\n",
    "            print_items(\n",
    "                    episode = episode,\n",
    "                    average_returns = average_returns[-1],\n",
    "                    timesteps = timesteps,\n",
    "                )\n",
    "            env.render(file_path=f\"{save_path}{episode}.gif\", type=\"history\")\n",
    "            \n",
    "        if timesteps % decay_interval == 0:\n",
    "            agent.decay_action_std(decay_rate, min_action_std=min_action_std)\n",
    "            \n",
    "        if timesteps % save_interval == 0 and timesteps > 0:\n",
    "            if average_returns[-1] > running_average_rewards:\n",
    "                print(f\"Average return: {average_returns[-1]}, running average: {running_average_rewards}\")\n",
    "                agent.save(save_path, episode=timesteps)\n",
    "\n",
    "    return average_returns\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     config_path = 'config/config.json'\n",
    "#     env, agent = initialize(config_path)\n",
    "#     average_returns = train_agent(env, agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config/config.json'\n",
    "env, agent = initialize(config_path)\n",
    "config = parse_config(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "actions = get_action(obs, agent, env)\n",
    "# mutiply actions by random number between - and 1\n",
    "actions = actions * np.random.uniform(-1, 1, size=actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = get_action(obs, agent, env)\n",
    "print(actions)\n",
    "# mutiply actions by random number between - and 1\n",
    "actions = actions * np.random.uniform(-1, 1, size=actions.shape)\n",
    "print(actions)\n",
    "obs, reward, agents_done, info = env.step(actions)\n",
    "print(env._get_actual_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gbest_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state[env.best_agent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(file_path='test.gif', type=\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_episodes = 2000\n",
    "# n_steps = env.ep_length\n",
    "# average_returns = []\n",
    "# timesteps = 0\n",
    "# update_timestep = 25\n",
    "# decay_timestep = 100\n",
    "# decay_rate = config['decay_rate']\n",
    "# print('decay rate: {}'.format(decay_rate))\n",
    "\n",
    "# average_returns = train_agent(env, agent, n_episodes=n_episodes, update_timestep=update_timestep, decay_rate=decay_rate, log_interval=200, min_action_std=0.1,\n",
    "#                 decay_interval=decay_timestep, save_interval=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE THE TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model_path = \"training_runs/2023-10-19_10-36-32/policy-16000.pth\"\n",
    "agent = MAPPO(config_path)\n",
    "agent.load(model_path)\n",
    "print(agent.policy.action_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, agent, n_steps):\n",
    "    observation_info = env.reset()\n",
    "    observation, observation_std = observation_info\n",
    "    done = False\n",
    "    best_agent_trajectory = np.zeros(shape=(env.ep_length, env.n_dim+1))\n",
    "    for step in range(n_steps):\n",
    "        actions = np.zeros((env.n_agents, env.n_dim))\n",
    "        for dim in range(env.n_dim):\n",
    "            # convert observation and std to float32\n",
    "            observation[dim] = observation[dim].astype(np.float32)\n",
    "            observation_std[dim] = observation_std[dim].astype(np.float32)\n",
    "            action = agent.select_action(observation[dim], observation_std[dim])\n",
    "            #print(agent.policy.action_var)\n",
    "            actions[:, dim] = action\n",
    "        # Go to the next state\n",
    "        observation_info, reward, done, info = env.step(actions)\n",
    "        best_agent_trajectory[step] = env._get_actual_state()[env.best_agent]\n",
    "        observation, observation_std = observation_info\n",
    "        # if done[0]:\n",
    "        #     break\n",
    "    return env, best_agent_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, best_trajectory = test(env, agent, 25)\n",
    "best_trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_vals = best_trajectory[:,-1]\n",
    "plt.plot(opt_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(type=\"history\", file_path=\"test.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPISODE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training_runs/2023-10-14_22-22-16/1200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "# set dpi and plot style\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract numerical values from the action strings\n",
    "def extract_action_values(action_str):\n",
    "    numbers = re.findall(r\"-?\\d+\\.\\d+\", action_str)\n",
    "    return [float(num) for num in numbers]\n",
    "\n",
    "# Function to create trend plots for observations and actions\n",
    "def plot_trends(agent_id, data, obs_string1='obs_2_prev_diff_dim_1', obs_string2='obs_10_prev_diff_dim_2'):\n",
    "    agent_data = data[data['agent'] == agent_id]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(agent_data['step'], agent_data[f'{obs_string1}'], label=f'{obs_string1}', color='r', linestyle='-', marker='o')\n",
    "    plt.plot(agent_data['step'], agent_data['action_dim1'], label='action_dim1', color='g', linestyle='--', marker='x')\n",
    "    plt.plot(agent_data['step'], agent_data[f'{obs_string2}'], label=f'{obs_string2}', color='b', linestyle='-.', marker='s')\n",
    "    plt.plot(agent_data['step'], agent_data['action_dim2'], label='action_dim2', color='m', linestyle=':', marker='d')\n",
    "    plt.plot(agent_data['step'], agent_data['scaled_value'], label='scaled_value', color='c', linestyle='-', marker='*')\n",
    "    \n",
    "    plt.title(f'Trend Plot for Agent {agent_id}')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to create path plots in scaled dimensions\n",
    "def plot_paths(data, dimension):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    agents = data['agent'].unique()\n",
    "    for agent in agents:\n",
    "        agent_data = data[data['agent'] == agent]\n",
    "        plt.plot(agent_data['step'], agent_data[f'scaled_{dimension}'], label=f'Agent {agent}', marker='o')\n",
    "        \n",
    "    plt.title(f'Agent Paths in Scaled {dimension.upper()}')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel(f'Scaled {dimension.upper()}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to create value evolution plots\n",
    "def plot_values(data):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    agents = data['agent'].unique()\n",
    "    for agent in agents:\n",
    "        agent_data = data[data['agent'] == agent]\n",
    "        plt.plot(agent_data['step'], agent_data['scaled_value'], label=f'Agent {agent}', marker='o')\n",
    "        \n",
    "    plt.title('Agent Value Evolution')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Scaled Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def correlation_with_actions(data, dimension):\n",
    "    # Calculating the correlation of all observations with actions in both dimensions\n",
    "    correlation_with_actions = data.filter(regex='obs').apply(lambda x: x.corr(data[f'action_dim{dimension}']))\n",
    "    \n",
    "    # Combining the correlations for both dimensions into a DataFrame\n",
    "    correlation_df = pd.DataFrame({\n",
    "        'Observation': correlation_with_actions.index,\n",
    "        f'Correlation_with_Action_Dim{dimension}': correlation_with_actions.values\n",
    "    })\n",
    "    \n",
    "    # Displaying the correlations\n",
    "    return correlation_df.sort_values(by=f'Correlation_with_Action_Dim{dimension}', ascending=False)\n",
    "\n",
    "def plot_correlations(data, dimension):\n",
    "    correlation_df = correlation_with_actions(data, dimension)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_df, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    plt.title(f'Correlation of Observations with Actions in Dimension {dimension}')\n",
    "    plt.xlabel('Actions')\n",
    "    plt.ylabel('Observations')\n",
    "    plt.xticks(ticks=[0.5], labels=[f'Action_Dim{dimension}'])\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to conduct the analysis\n",
    "def analyze_agents(dataset_path, num_agents_to_plot=4):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    data[['action_dim1', 'action_dim2']] = pd.DataFrame(data['action'].apply(extract_action_values).tolist(), index=data.index)\n",
    "    \n",
    "    for agent in range(num_agents_to_plot):\n",
    "        plot_trends(agent, data)\n",
    "    \n",
    "    plot_paths(data, 'dim1')\n",
    "    plot_paths(data, 'dim2')\n",
    "    plot_values(data)\n",
    "    display_correlations(data)\n",
    "\n",
    "\n",
    "# Function to calculate and display correlation coefficients\n",
    "def display_correlations(data):\n",
    "    dim1_columns = [col for col in data.columns if \"obs\" in col and \"dim_1\" in col]\n",
    "    value_columns = [col for col in data.columns if \"obs\" in col and \"value\" in col]\n",
    "    dim1_features = dim1_columns + value_columns + ['action_dim1']\n",
    "\n",
    "    dim2_columns = [col for col in data.columns if \"obs\" in col and \"dim_2\" in col]\n",
    "    dim2_features = dim2_columns + value_columns + ['action_dim2']\n",
    "    \n",
    "    correlation_dim1 = data[dim1_features].corr()['action_dim1']\n",
    "    correlation_dim2 = data[dim2_features].corr()['action_dim2']\n",
    "    \n",
    "    print(\"Correlation Coefficients for Dimension 1:\")\n",
    "    print(correlation_dim1)\n",
    "    print(\"\\nCorrelation Coefficients for Dimension 2:\")\n",
    "    print(correlation_dim2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'training_runs/2023-10-14_23-51-37/1200.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "data[['action_dim1', 'action_dim2']] = pd.DataFrame(data['action'].apply(extract_action_values).tolist(), index=data.index)\n",
    "obs1 = \"obs_3_prev_diff_value\"\n",
    "obs2 = \"obs_5_pbest_diff_value\"\n",
    "num_agents_to_plot = 4\n",
    "for agent in range(num_agents_to_plot):\n",
    "    plot_trends(agent, data, obs_string1=obs1, obs_string2=obs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_paths(data, 'dim1')\n",
    "plot_paths(data, 'dim2')\n",
    "plot_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_correlations(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eloEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
