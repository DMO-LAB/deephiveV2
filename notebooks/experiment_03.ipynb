{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import os\n",
    "from deephive.environment.deephive_utils import *\n",
    "from deephive.environment.utils import *\n",
    "import numpy as np \n",
    "from deephive.exploration.gp_surrogate import GPSurrogateModule\n",
    "from deephive.environment.utils import filter_points\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import neptune\n",
    "import torch \n",
    "\n",
    "# from deephive.environment.utils import num_function_evaluation, plot_individual_function_evaluation, plot_num_function_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Experiment 1: VARIANCE DRIVEN EXPLORERS AND EXPLOITERS\n",
    "config_path = \"../config/train_config.json\"\n",
    "model_path = \"../models/pbest_unfreeze.pth\"\n",
    "model_path_2 = \"../models/gbest.pth\"\n",
    "\n",
    "config = parse_config(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_logger(api_token, config, tag=\"\"):\n",
    "        run = neptune.init_run(\n",
    "        project=\"DMO-LAB/DeepHive-V2\",\n",
    "        source_files=[\"environment\", \"policies\", \"deephive.py\", \"config\"],\n",
    "        api_token=api_token,\n",
    "        tags=[config[\"mode\"], config[\"objective_function\"], str(config[\"layer_size\"]), tag]\n",
    "        )\n",
    "        neptune_logger = run\n",
    "        return neptune_logger\n",
    "\n",
    "\n",
    "def get_action(observation_info, agent_policy, env, observation_std=None, debug=False):\n",
    "    observation = observation_info\n",
    "    # enforce observation_std to be a numpy array\n",
    "    if observation_std is None:\n",
    "        observation_std = np.zeros_like(observation)\n",
    "    else:\n",
    "        observation_std = np.array(observation_std)\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    for dim in range(env.n_dim):\n",
    "        observation[dim] = observation[dim]#.astype(np.float32)\n",
    "        if observation_std is not None:\n",
    "            observation_std[dim] = observation_std[dim]#.astype(np.float32) \n",
    "        else:\n",
    "            observation_std[dim] = np.zeros_like(observation[dim])\n",
    "        action = agent_policy.select_action(observation[dim], observation_std[dim])\n",
    "        actions[:, dim] = action\n",
    "    return actions\n",
    "\n",
    "def get_std_obs(env):\n",
    "    states = env.state\n",
    "    scaled_gbest = env.scaler_helper.scale(env.gbest[:env.n_dim],env.min_pos, env.max_pos)\n",
    "    mean, var = np.mean(states, axis=0), np.var(states, axis=0)\n",
    "    std_obss = []\n",
    "    for i in range(env.n_dim):\n",
    "        agent_std_obs = []\n",
    "        for agent, agent_state in enumerate(states):\n",
    "            obs = [\n",
    "                var[i],\n",
    "                agent_state[i] - mean[i],\n",
    "                agent_state[env.n_dim] - mean[env.n_dim],\n",
    "                agent_state[i] - scaled_gbest[i],\n",
    "                agent_state[env.n_dim] - 1,\n",
    "                agent_state[i] - env.prev_state[agent][i],\n",
    "                agent_state[env.n_dim] - env.prev_state[agent][env.n_dim],\n",
    "            ]\n",
    "            agent_std_obs.append(np.abs(np.array(obs)))\n",
    "        std_obss.append(np.array(agent_std_obs))\n",
    "        \n",
    "    return std_obss\n",
    "\n",
    "def train_agent(env, agent, neptune_logger=None, save_dir=None, **kwargs):\n",
    "    # Setting default values if not provided\n",
    "    n_episodes = kwargs.get('n_episodes', 2000)\n",
    "    update_timestep = kwargs.get('update_timestep', 20)\n",
    "    decay_rate = kwargs.get('decay_rate', 0.95)\n",
    "    log_interval = kwargs.get('log_interval', 500)\n",
    "    decay_interval = kwargs.get('decay_interval', 1000)\n",
    "    save_interval = kwargs.get('save_interval', 5000)\n",
    "    min_action_std = kwargs.get('std_min', 0.02)\n",
    "    debug = kwargs.get('debug', False)\n",
    "    #print(debug)\n",
    "    \n",
    "    average_returns = []\n",
    "    if not save_dir:\n",
    "        training_run_title = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        save_path = f\"training_runs/{training_run_title}/\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "    else:\n",
    "        save_path = save_dir\n",
    "    timesteps = 0\n",
    "    for episode in range(0, n_episodes+1):\n",
    "        observation_info, role = env.reset()\n",
    "        agent.policy.reset_min_std()\n",
    "        obs_std = get_std_obs(env)\n",
    "        episode_return = np.zeros(env.n_agents)\n",
    "        for step in range(env.ep_length):\n",
    "            actions = get_action(observation_info, agent, env, observation_std=obs_std, debug=debug)\n",
    "            observation_info, reward, done, info = env.step(actions)\n",
    "            observation_info, role = observation_info\n",
    "            obs_std = get_std_obs(env)\n",
    "            #print(obs_std)\n",
    "            # add reward to agent buffer\n",
    "            for ag in range(env.n_agents):\n",
    "                agent.buffer.rewards += [reward[ag]] * env.n_dim\n",
    "                agent.buffer.is_terminals += [done[ag]] * env.n_dim\n",
    "            \n",
    "            if neptune_logger:\n",
    "                neptune_logger[f\"train/mean_predicted_action_std\"].log(agent.policy.mean_predicted_action_std)\n",
    "                neptune_logger[f\"train/mean_min_action_std\"].log(agent.policy.mean_min_action_std)\n",
    "                neptune_logger[f\"train/mean_applied_action_var\"].log(agent.policy.mean_applied_action_vars)\n",
    "            # if episode % log_interval == 0:\n",
    "            #     print(f\"Mean predicted action std: {agent.policy.mean_predicted_action_std}, mean min action std: {agent.policy.mean_min_action_std}\")\n",
    "            \n",
    "            episode_return += reward\n",
    "            if neptune_logger and episode % log_interval == 0:\n",
    "                # log global best agent value\n",
    "                neptune_logger[f\"train/global_best_value/episode{episode}\"].log(float(info[\"gbest\"][-1]))\n",
    "            \n",
    "            if step == env.ep_length - 1:\n",
    "                average_returns.append(np.mean(episode_return))\n",
    "                running_average_rewards = np.mean(average_returns)\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[\"train/average_return\"].log(average_returns[-1])\n",
    "\n",
    "                \n",
    "            timesteps += 1\n",
    "        if timesteps % update_timestep == 0:\n",
    "            agent.update()\n",
    "    \n",
    "        if episode % log_interval == 0 and timesteps > 0:\n",
    "            print_items(\n",
    "                    episode = episode,\n",
    "                    average_returns = average_returns[-1],\n",
    "                    timesteps = timesteps,\n",
    "                )\n",
    "            # predicted_action_std = [agent.policy.std_actor(torch.tensor(obs).float()) for obs in obs_std]\n",
    "            # print(predicted_action_std)\n",
    "            env.render(file_path=f\"{save_path}{episode}.gif\", type=\"history\")\n",
    "            if neptune_logger:\n",
    "                neptune_logger[f\"train/gifs/{episode}.gif\"].upload(f\"{save_path}{episode}.gif\")\n",
    "\n",
    "            \n",
    "        if timesteps % decay_interval == 0:\n",
    "            agent.decay_action_std(decay_rate, min_action_std=min_action_std,debug=True)\n",
    "            \n",
    "        if timesteps % save_interval == 0 and timesteps > 0:\n",
    "            if average_returns[-1] > running_average_rewards:\n",
    "                print(f\"Average return: {average_returns[-1]}, running average: {running_average_rewards}\")\n",
    "                agent.save(save_path, episode=timesteps)\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[f\"train/checkpoints/timesteps-{timesteps}\"].upload(f\"{save_path}/policy-{timesteps}.pth\")\n",
    "    neptune_logger.stop()\n",
    "    return average_returns, agent, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, iters, save_dir=None, **kwargs):\n",
    "    log_interval = kwargs.get('log_interval', 200)\n",
    "    decay_interval = kwargs.get('decay_interval', 1000)\n",
    "    test_std = kwargs.get('test_std', 0.02)\n",
    "    decay_rate = kwargs.get('decay_rate', 0.9)\n",
    "    min_action_std = kwargs.get('std_min', 0.02)\n",
    "    \n",
    "    \n",
    "    if not save_dir:\n",
    "        training_run_title = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        save_path = f\"testing_runs/{training_run_title}/\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "    else:\n",
    "        save_path = save_dir\n",
    "    timesteps = 0\n",
    "    all_gbests = []\n",
    "    for episode in range(0, iters):\n",
    "        observation_info, role = env.reset()\n",
    "        episode_gbests = []\n",
    "        agent.set_action_std(test_std)\n",
    "        for step in range(env.ep_length):\n",
    "            actions = get_action(observation_info, agent, env, observation_std=role)\n",
    "            observation_info, reward, done, info = env.step(actions)\n",
    "            observation_info, role = observation_info\n",
    "            episode_gbests.append(info[\"gbest\"][-1])\n",
    "            if decay_interval and timesteps % decay_interval == 0:\n",
    "                agent.decay_action_std(decay_rate, min_action_std=min_action_std)\n",
    "            \n",
    "        if log_interval and episode % log_interval == 0:\n",
    "            _ = env.render(file_path=f\"{save_path}{episode}.gif\", type=\"history\")\n",
    "        \n",
    "        \n",
    "        all_gbests.append(episode_gbests)\n",
    "        \n",
    "    np.save(f\"{save_path}gbests.npy\", np.array(all_gbests))\n",
    "    all_gbests = np.array(all_gbests)\n",
    "    title = f\"Test run, std\"\n",
    "    opt_value = env.opt_value\n",
    "    num_function_evaluation(fopt=all_gbests ,n_agents=env.n_agents, save_dir=save_path + \"num_function_evaluations.png\", opt_value=opt_value,\n",
    "                                    log_scale=False, plot_error_bounds=True, title=title)\n",
    "            \n",
    "    plot_individual_function_evaluation(fopt=all_gbests ,n_agents=env.n_agents, save_dir=save_path + \"num_function_evaluations2.png\", opt_value=opt_value,\n",
    "                                    log_scale=False, title=title)\n",
    "\n",
    "    \n",
    "    return all_gbests, save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 62\n",
    "exp_name = \"exp_\" + str(exp_num)\n",
    "result_path = 'experiments/training/results/' + exp_name + '/'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "api_token = os.environ.get(\"NEPTUNE_API_TOKEN\")\n",
    "config['freeze'] = True\n",
    "config['use_gbest'] = True\n",
    "# config['variable_std'] = False\n",
    "config['action_std'] = 0.5\n",
    "config['decay_rate'] = 0.97\n",
    "config['log_interval'] = 500\n",
    "config['std_min'] = 0.02\n",
    "config['use_optimal_value'] = True\n",
    "config['tol'] = .99\n",
    "config['ep_length'] = 20\n",
    "config['n_agents'] = 10\n",
    "config['n_dim'] = 2\n",
    "config['objective_function'] = \"CosineMixtureFunction\"\n",
    "config['reward_scheme'] = \"FullRewardScheme\"\n",
    "mode = \"train\"\n",
    "config[\"save_interval\"] = 2500\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"lr\"] = 0.0001\n",
    "config[\"layer_size\"] = [16, 16]\n",
    "config[\"variable_std\"] = True\n",
    "config['mode'] = mode   \n",
    "config[\"use_std_model\"] = True\n",
    "config[\"std_model_layers\"] = [16, 16]\n",
    "config[\"decay_interval\"] = 1000\n",
    "# config[\"decay_rate\"] = 0.95\n",
    "config[\"std_obs_dim\"] = 7\n",
    "config[\"negative\"] = True\n",
    "config[\"function_id\"] = 0\n",
    "config['update_timestep'] = 1000\n",
    "config[\"use_grid\"] = False\n",
    "env, agent_policy = initialize(config, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agent = 0\n",
    "# observation_info = env.reset()\n",
    "# observation_info, role = observation_info\n",
    "# obs_std = get_std_obs(env)\n",
    "# agent = agent_policy\n",
    "# actions = get_action(observation_info, agent, env, observation_std=obs_std, debug=True)\n",
    "# observation_info, reward, done, info = env.step(actions)\n",
    "\n",
    "# import torch.distributions as tdist\n",
    "# obs_std = np.array(obs_std)\n",
    "\n",
    "# action_mean = agent_policy.policy.actor(torch.tensor(obs[0]).float())\n",
    "# action_std = 0.2*agent_policy.policy.std_actor(torch.tensor(obs_std[0]).float())\n",
    "\n",
    "# dist = tdist.Normal(action_mean, action_std[2])\n",
    "# action = dist.sample()\n",
    "# dist.log_prob(action).sum(dim=1)\n",
    "# obs = np.array(observation_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"learning_std\"\n",
    "neptune_logger = initialize_logger(api_token, config, tag=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(env, agent_policy, neptune_logger=neptune_logger, save_dir=result_path, **config, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.307"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_policy.action_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/DMO-LAB/DeepHive-V2/e/DEEP-118/metadata\n"
     ]
    }
   ],
   "source": [
    "neptune_logger.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.105 , 0.405 , 0.4656],\n",
       "       [0.27  , 0.755 , 0.6095],\n",
       "       [0.685 , 0.775 , 0.6142],\n",
       "       [0.085 , 0.95  , 0.    ],\n",
       "       [0.21  , 0.405 , 0.5239],\n",
       "       [0.01  , 0.35  , 0.1642],\n",
       "       [0.1   , 0.39  , 0.4512],\n",
       "       [0.255 , 0.38  , 0.6503],\n",
       "       [0.285 , 0.08  , 0.4282],\n",
       "       [0.3   , 0.6   , 0.7515]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = env.reset()\n",
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3.195225e-02, 1.255000e-01, 2.600000e-04, 1.950000e-01,\n",
       "         5.344000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 3.950000e-02, 1.436400e-01, 3.000000e-02,\n",
       "         3.905000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 4.545000e-01, 1.483400e-01, 3.850000e-01,\n",
       "         3.858000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 1.455000e-01, 4.658600e-01, 2.150000e-01,\n",
       "         1.000000e+00, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 2.050000e-02, 5.804000e-02, 9.000000e-02,\n",
       "         4.761000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 2.205000e-01, 3.016600e-01, 2.900000e-01,\n",
       "         8.358000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 1.305000e-01, 1.466000e-02, 2.000000e-01,\n",
       "         5.488000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 2.450000e-02, 1.844400e-01, 4.500000e-02,\n",
       "         3.497000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 5.450000e-02, 3.766000e-02, 1.500000e-02,\n",
       "         5.718000e-01, 0.000000e+00, 0.000000e+00],\n",
       "        [3.195225e-02, 6.950000e-02, 2.856400e-01, 0.000000e+00,\n",
       "         2.485000e-01, 0.000000e+00, 0.000000e+00]]),\n",
       " array([[5.9579e-02, 1.0400e-01, 2.6000e-04, 1.9500e-01, 5.3440e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 2.4600e-01, 1.4364e-01, 1.5500e-01, 3.9050e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 2.6600e-01, 1.4834e-01, 1.7500e-01, 3.8580e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 4.4100e-01, 4.6586e-01, 3.5000e-01, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 1.0400e-01, 5.8040e-02, 1.9500e-01, 4.7610e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 1.5900e-01, 3.0166e-01, 2.5000e-01, 8.3580e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 1.1900e-01, 1.4660e-02, 2.1000e-01, 5.4880e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 1.2900e-01, 1.8444e-01, 2.2000e-01, 3.4970e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 4.2900e-01, 3.7660e-02, 5.2000e-01, 5.7180e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.9579e-02, 9.1000e-02, 2.8564e-01, 0.0000e+00, 2.4850e-01,\n",
       "         0.0000e+00, 0.0000e+00]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_std_obs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6740],\n",
       "        [0.6732],\n",
       "        [0.6739],\n",
       "        [0.6653],\n",
       "        [0.6731],\n",
       "        [0.6690],\n",
       "        [0.6738],\n",
       "        [0.6720],\n",
       "        [0.6753],\n",
       "        [0.6720]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "obs_std = get_std_obs(env)\n",
    "agent_policy.policy.std_actor(torch.tensor(obs_std[0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291],\n",
      "        [0.7291]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732],\n",
      "        [0.6732]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(agent_policy.policy.std_actor(torch.ones_like(torch.tensor(obs_std[1]).float())))\n",
    "print(agent_policy.policy.std_actor(torch.zeros_like(torch.tensor(obs_std[1]).float())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_policy.policy.std_actor.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_policy.policy.std_actor.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_config = config.copy()\n",
    "config['freeze'] = False\n",
    "config['use_gbest'] = False\n",
    "config['variable_std'] = False\n",
    "config['test_std'] = 0.2\n",
    "config['decay_interval'] = 2\n",
    "config['log_interval'] = 10 \n",
    "config['decay_rate'] = 0.8 \n",
    "config['std_min'] = 0.00001\n",
    "config['use_optimal_value'] = False\n",
    "config['ep_length'] = 20 \n",
    "config['n_agents'] = 10 \n",
    "config['n_dim'] = 2\n",
    "config['objective_function'] = \"GaussianPeakFunction\"\n",
    "mode = \"test\" \n",
    "config[\"iters\"] = 100 \n",
    "config[\"layer_size\"] = [32, 32]\n",
    "config['mode'] = mode   \n",
    "exp_num = 27\n",
    "model_timestep = 40000\n",
    "model_path = f\"experiments/training/results/exp_{exp_num}/policy-{model_timestep}.pth\"\n",
    "env, agent_policy = initialize(config, mode=mode, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_gbests, save_dir = test_agent(env, agent_policy, **config, save_dir=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_policy.policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rosenbrock_function_modified(params: np.ndarray, x_opt: np.ndarray = None, f_opt: float = 0) -> np.ndarray:\n",
    "    if x_opt is None:\n",
    "        x_opt = np.ones(params.shape[1])  # Assuming x_opt = 1 for all dimensions\n",
    "    \n",
    "    D = params.shape[1]\n",
    "\n",
    "    # Compute the scaling factor\n",
    "    scaling_factor = max(1, np.sqrt(D / 8))\n",
    "    \n",
    "    # Transform x to z\n",
    "    z = scaling_factor * (params - x_opt) + 1\n",
    "    \n",
    "    # Apply the Rosenbrock formula\n",
    "    summands = 100.0 * (z[:, :-1]**2 - z[:, 1:])**2 + (z[:, :-1] - 1)**2\n",
    "    result = np.sum(summands, axis=1)\n",
    "    \n",
    "    # Since you're maximizing, negate the function (subtract from f_opt if used)\n",
    "    return -(result + f_opt)\n",
    "\n",
    "# Example usage\n",
    "D = 3 # Dimensionality of the problem\n",
    "params = np.array([[1, 1, 1], [0, 0, 0]])  # Example parameters\n",
    "\n",
    "rosenbrock_values = rosenbrock_function_modified(params)\n",
    "print(rosenbrock_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bent_cigar_function(params: np.ndarray, f_opt: float = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bent Cigar Function.\n",
    "    \n",
    "    Args:\n",
    "        params (np.ndarray): Parameters for the Bent Cigar function.\n",
    "        f_opt (float): Optimal function value shift.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Function values.\n",
    "    \"\"\"\n",
    "    # Assuming z = x - x_opt and x_opt = 0 for simplicity\n",
    "    z = params  # This would be different if rotation and asymmetry were applied\n",
    "    \n",
    "    # Calculate the function value\n",
    "    term1 = z[:, 0]**2\n",
    "    term2 = 10**6 * np.sum(z[:, 1:]**2, axis=1)\n",
    "    result = term1 + term2 + f_opt\n",
    "    \n",
    "    return -result\n",
    "\n",
    "# Example usage\n",
    "D = 2  # Dimensionality of the problem\n",
    "params = np.random.randn(10, D)  # Example parameters: 10 sets of D-dimensional inputs\n",
    "print(params)\n",
    "bent_cigar_values = bent_cigar_function(params)\n",
    "print(bent_cigar_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schaffers_f7_function(params: np.ndarray, f_opt: float = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Schaffer's F7 Function.\n",
    "    \n",
    "    Args:\n",
    "        params (np.ndarray): Parameters for the Schaffer's F7 function.\n",
    "        f_opt (float): Optimal function value shift.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Function values.\n",
    "    \"\"\"\n",
    "    D = params.shape[1]  # Dimensionality of the input\n",
    "    \n",
    "    # Calculate s_i\n",
    "    z = params  # Assuming z = x for simplicity\n",
    "    s_i = np.sqrt(z[:, :-1]**2 + z[:, 1:]**2)\n",
    "    \n",
    "    # Calculate the function value\n",
    "    term = (np.sqrt(s_i) + np.sqrt(s_i) * np.sin(50 * s_i**0.2)**2)**2\n",
    "    result = np.mean(term, axis=1) + f_opt  # Assuming f_opt includes f_pen(x) if necessary\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "D = 5  # Dimensionality of the problem\n",
    "params = np.random.randn(10, D)  # Example parameters: 10 sets of D-dimensional inputs\n",
    "\n",
    "schaffers_f7_values = schaffers_f7_function(params)\n",
    "print(schaffers_f7_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
