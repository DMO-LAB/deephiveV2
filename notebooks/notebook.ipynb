{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deephive.environment.utils import parse_config\n",
    "from datetime import datetime\n",
    "import os \n",
    "import numpy as np\n",
    "import neptune\n",
    "import torch\n",
    "from plot_utils import *\n",
    "from deephive.environment.optimization_environment import OptimizationEnv\n",
    "from deephive.environment.optimization_functions.benchmark_functions import FunctionSelector\n",
    "from deephive.environment.utils import mean_confidence_interval\n",
    "import pandas as pd\n",
    "function_selector = FunctionSelector()\n",
    "from deephive.policies.mappo import MAPPO\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "from dotenv import load_dotenv\n",
    "api_token = os.environ.get(\"NEPTUNE_API_TOKEN\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPHIVE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/exp_config.json\"\n",
    "model_path = \"../models/pbest_unfreeze.pth\"\n",
    "model_path_2 = \"../models/gbest.pth\"\n",
    "config = parse_config(config_path)\n",
    "config['use_gbest'] = False\n",
    "config['use_lbest'] = False\n",
    "config[\"use_optimal_value\"] = True\n",
    "config[\"log_scale\"] = True\n",
    "config[\"include_gbest\"] = False\n",
    "config[\"negative\"] = True\n",
    "if config[\"include_gbest\"] or config[\"use_lbest\"]:\n",
    "    config[\"obs_dim\"] = 5\n",
    "else:\n",
    "    config[\"obs_dim\"] = 4\n",
    "config[\"ep_length\"] = 25\n",
    "\n",
    "config[\"min_action_std\"] = 0.001\n",
    "config[\"action_std\"] = 0.2\n",
    "config[\"variable_std\"] = False\n",
    "config[\"update_timestep\"] = 2\n",
    "config[\"decay_rate\"] = 0.99\n",
    "config[\"log_interval\"] = 500\n",
    "config[\"decay_interval\"] = 5\n",
    "config[\"save_interval\"] = 200\n",
    "config[\"test_decay_rate\"] = 0.9\n",
    "config[\"test_decay_start\"] = 0\n",
    "config[\"reward_scheme\"] = \"FullRewardScheme2\"\n",
    "config[\"observation_scheme\"]= \"SimpleObservationScheme\"\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"plot_gif\"] = True\n",
    "config[\"plot_gbest\"] = True\n",
    "config[\"test_ep_length\"] = 100\n",
    "config[\"n_agents\"] = 12\n",
    "config[\"n_dim\"] = 2\n",
    "config['objective_function'] = \"BenchmarkFunctions\" \n",
    "config[\"function_id\"] = \"f01\"\n",
    "config[\"neighborhood_size\"] = 4\n",
    "config[\"topology\"] = \"random\"\n",
    "\n",
    "mode = \"train\"\n",
    "\n",
    "def initialize(config, mode=\"train\", **kwargs):\n",
    "    env = OptimizationEnv(config)\n",
    "    agent_policy = MAPPO(config)\n",
    "    if mode == \"test\" or mode == \"benchmark\":\n",
    "        model_path = kwargs.get(\"model_path\", None)\n",
    "        if model_path is None:\n",
    "            raise ValueError(\"Model path must be provided for testing\")\n",
    "        # check if model path is a list of paths\n",
    "        if isinstance(model_path, list):\n",
    "            agent_policies = []\n",
    "            for path in model_path:\n",
    "                agent_policy = MAPPO(config)\n",
    "                agent_policy.load(path)\n",
    "                agent_policies.append(agent_policy)\n",
    "            return env, agent_policies\n",
    "        else:\n",
    "            agent_policy.load(model_path)\n",
    "            return env, agent_policy\n",
    "    else:\n",
    "        return env, agent_policy\n",
    "    \n",
    "\n",
    "def get_action(observation, agent_policy, env, observation_std=None, **kwargs):\n",
    "    # Ensure observation_info is a numpy array\n",
    "    \n",
    "    if not isinstance(observation, np.ndarray):\n",
    "        observation = np.array(observation)\n",
    "        assert observation.shape[0] == env.n_dim, \"Observation must have the same number of dimensions as the environment\"\n",
    "\n",
    "    # Initialize observation_std with zeros or use provided std, ensuring it matches the shape of observation\n",
    "    if observation_std is None:\n",
    "        observation_std = np.zeros_like(observation)\n",
    "    else:\n",
    "        observation_std = np.array(observation_std)\n",
    "\n",
    "    # Flatten the observation and std arrays\n",
    "    observation_flat = observation.reshape(env.n_dim * env.n_agents, -1)  # Flatten to 1D array\n",
    "    observation_std_flat = observation_std.reshape(-1)  # Flatten to 1D array\n",
    "    # Pass the entire flattened observation and std arrays to select_action\n",
    "    action_flat = agent_policy.select_action(observation_flat, observation_std_flat)\n",
    "\n",
    "    # Reshape the flattened action array back to the original (n_agents, n_dim) shape\n",
    "    actions = action_flat.reshape(env.n_dim, env.n_agents).T  # Reshape to (n_agents, n_dim\n",
    "\n",
    "    return actions  # Return the action\n",
    "\n",
    "\n",
    "def get_direct_action(obs, obs_std, agent_policy):\n",
    "    torch_obs = torch.FloatTensor(obs)\n",
    "    torch_obs_std = torch.FloatTensor(obs_std)\n",
    "    action = agent_policy.policy.act(torch_obs, torch_obs_std)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, agent_policy = initialize(config, mode=\"train\", model_path=None)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_logger(api_token, tags, config, mode=\"train\"):\n",
    "        run = neptune.init_run(\n",
    "        project=\"DMO-LAB/DeepHive-V2\",\n",
    "        # source files = all python files in the current directory,\n",
    "        source_files=[\"*.py\"],\n",
    "        api_token=api_token,\n",
    "        tags=[tags, mode, config[\"objective_function\"], str(config[\"layer_size\"])]\n",
    "        )\n",
    "        return run\n",
    "\n",
    "def train(env, agent_policy, config, title=\"experiment_1\", **kwargs):\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "        \n",
    "    save_path = kwargs.get(\"save_path\", \"training_results/\")\n",
    "    save_path = os.path.join(save_path, title)\n",
    "    # make directory if it does not exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    n_episodes = config[\"n_episodes\"]\n",
    "    average_returns = []  \n",
    "    timestep = 0    \n",
    "    for i in range(n_episodes):\n",
    "        #print(f\"Episode {i} started, timestep {timestep}\")\n",
    "        obs, obs_std = env.reset()\n",
    "        episode_return = np.zeros(env.n_agents)\n",
    "        for step in range(env.ep_length):\n",
    "            #print(f\"Episode {i}, step {step}, timestep {timestep}\")\n",
    "            actions = get_action(obs, agent_policy, env, obs_std)\n",
    "            obs, reward, done, info = env.step(actions)\n",
    "            for ag in range(env.n_agents):\n",
    "                agent_policy.buffer.rewards += [reward[ag]] * env.n_dim\n",
    "                agent_policy.buffer.is_terminals += [done[ag]] * env.n_dim\n",
    "            episode_return += reward\n",
    "            obs, obs_std = obs\n",
    "            timestep += 1\n",
    "            if step == env.ep_length - 1:\n",
    "                average_returns.append(np.mean(episode_return))\n",
    "                if neptune_logger is not None:\n",
    "                    neptune_logger[\"train/average_return\"].log(average_returns[-1])\n",
    "                running_average_rewards = np.mean(average_returns)\n",
    "                \n",
    "            if neptune_logger and i % config[\"log_interval\"] == 0:\n",
    "                neptune_logger[f\"Episode_{i}/gbest_values\"].log(env.gbest[-1])\n",
    "\n",
    "        if i % config[\"update_timestep\"] == 0 and timestep > 0:\n",
    "            #print(f\"Updating policy at episode {i}\")\n",
    "            agent_policy.update()\n",
    "        if i % config[\"log_interval\"] == 0 and timestep > 0:\n",
    "            print(f\"Episode {i} completed\")\n",
    "            print(f\"Average return: {running_average_rewards}\")\n",
    "            if env.n_dim == 2:\n",
    "                env.render(type=\"history\", file_path=f\"{save_path}/episode_{i}.gif\")  \n",
    "                if neptune_logger:\n",
    "                    neptune_logger[f\"train/gifs/{i}.gif\"].upload(f\"{save_path}/episode_{i}.gif\")\n",
    "        if i % config[\"decay_interval\"] == 0 and timestep > 0:\n",
    "            agent_policy.decay_action_std(config[\"decay_rate\"], min_action_std=config[\"min_action_std\"], debug=False)\n",
    "        if i % config[\"save_interval\"] == 0 and timestep > 0:\n",
    "            if average_returns[-1] > running_average_rewards:\n",
    "                print(f\"Saving model at episode {i} with average return {average_returns[-1]} and running average {running_average_rewards}\")\n",
    "                agent_policy.save(save_path, episode=i)\n",
    "        \n",
    "    return agent_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = \"experiment_104\"\n",
    "# tags = \"training with 4 observation using pbest and log scale\"\n",
    "# neptune_logger = initialize_logger(api_token, title, config, mode=\"train\")\n",
    "# agent_policy = train(env, agent_policy, config, title=title, neptune_logger=neptune_logger)\n",
    "# neptune_logger.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPHIVE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, agent_policy, iters, decay_start=100, decay_rate=0.995, \n",
    "         min_action_std=0.001, max_action_std=0.5, \n",
    "         save_gif = False, save_path = \"test_results\", function_id=0, **kwargs):\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    all_gbest_vals = []\n",
    "    print(f\"Testing function {function_id} with {env.n_agents} agents and {env.n_dim} dimensions\")\n",
    "    for iter in range(iters):\n",
    "        gbest_vals = []\n",
    "        obs = env.reset()[0]\n",
    "        agent_policy.set_action_std(max_action_std)\n",
    "        current_action_std = agent_policy.action_std\n",
    "        for step in range(env.ep_length):\n",
    "            action = get_action(obs, agent_policy, env)\n",
    "            obs, _ , _ , _ = env.step(action)\n",
    "            obs = obs[0]\n",
    "            gbest_vals.append(env.gbest[-1])\n",
    "            if neptune_logger:\n",
    "                neptune_logger[f\"test/{function_id}/Episode_{iter}/gbest_values\"].log(env.gbest[-1])\n",
    "            if step >= decay_start:\n",
    "                # Decay the std uniformly from the max to the min std over the specified rate\n",
    "                current_action_std = max(min_action_std, current_action_std * decay_rate)\n",
    "                agent_policy.set_action_std(current_action_std)   \n",
    "        if env.n_dim == 2 and save_gif:\n",
    "            env.render(type=\"history\", file_path=f\"{save_path}/episode_{iter}.gif\") \n",
    "            if neptune_logger:\n",
    "                neptune_logger[f\"test/{function_id}/gifs/{iter}.gif\"].upload(f\"{save_path}/episode_{iter}.gif\")\n",
    "        all_gbest_vals.append(np.array(gbest_vals))\n",
    "        print(f\"Final gbest value: {env.gbest[-1]} at iteration {iter}\")\n",
    "    \n",
    "    np.save(f\"{save_path}/{function_id}_gbest_history.npy\", np.array(all_gbest_vals))\n",
    "    return all_gbest_vals\n",
    "\n",
    "def analyze_results(base_path, model_lists, model_path_list, successful_functions, function_selector, **kwargs):\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "    # Define a multi-level column structure: each optimizer has the same sub-columns\n",
    "    columns = pd.MultiIndex.from_product([model_lists, \n",
    "                                          [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]],\n",
    "                                         names=['optimizer', 'metric'])\n",
    "    # Initialize an empty DataFrame with these columns\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for function_id in successful_functions:\n",
    "        row_data = {}\n",
    "        function_info = function_selector.get_function(function_id)\n",
    "        function_opt_val = function_info[\"global_min\"]\n",
    "        \n",
    "        # Loop through each optimizer\n",
    "        for optimizer, model_path in zip(model_lists, model_path_list):\n",
    "            data_path = model_path + f\"deephive/{function_id}/{function_id}_gbest_history.npy\"\n",
    "            try:\n",
    "                # Attempt to load the optimizer's result and compute metrics\n",
    "                gbest_values = np.load(data_path) * -1\n",
    "                mean_val, lower_val, upper_val = mean_confidence_interval(gbest_values)\n",
    "                error_val = abs(mean_val[-1] - function_opt_val)\n",
    "                row_data[(optimizer, 'mean')] = mean_val[-1]\n",
    "                row_data[(optimizer, 'lower')] = lower_val[-1]\n",
    "                row_data[(optimizer, 'upper')] = upper_val[-1]\n",
    "                row_data[(optimizer, 'optimum')] = function_opt_val\n",
    "                row_data[(optimizer, 'error')] = error_val\n",
    "                \n",
    "                # Additional logging or plotting can be added here as needed\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {optimizer} for function {function_id}: {e}\")\n",
    "                # Fill missing values if any error occurs\n",
    "                for metric in [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]:\n",
    "                    row_data[(optimizer, metric)] = np.nan\n",
    "        \n",
    "        # After collecting data for all optimizers, add the row to the DataFrame\n",
    "        #df = df.append(pd.Series(row_data, name=function_id))\n",
    "        df.loc[function_id] = row_data\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(f\"{base_path}/results.csv\")\n",
    "    \n",
    "    # Log the DataFrame if Neptune logger is provided\n",
    "    if neptune_logger:\n",
    "        neptune_logger[\"test/results\"].upload(f\"{base_path}/results.csv\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def run_test(function_ids, iters, save_dir, model_path, config, **kwargs):\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "    successful_functions = []\n",
    "    for function_id in function_ids:\n",
    "        try:\n",
    "            config[\"function_id\"] = function_id\n",
    "            function_dim = function_selector.get_function(function_id)[\"dimension\"]\n",
    "            if config[\"n_dim\"] > function_dim:\n",
    "                print(f\"function {function_id} has {function_dim} dimensions, setting n_dim to {function_dim}\")\n",
    "                config[\"n_dim\"] = function_dim\n",
    "            env, agent_policy = initialize(config, mode=\"test\", model_path=model_path)\n",
    "            _ = env.reset()[0]\n",
    "            agent_policy.load(model_path)\n",
    "            save_path = f\"{save_dir}/{function_id}\"\n",
    "            try:\n",
    "                all_gbest_vals = test(env, agent_policy, iters, decay_start=0, decay_rate=0.9, \n",
    "                                min_action_std=0.0001, max_action_std=0.5, \n",
    "                                save_gif = False, save_path = save_path, function_id=function_id, neptune_logger=neptune_logger)\n",
    "                successful_functions.append(function_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Function {function_id} failed with error {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Function {function_id} failed with error {e}\")\n",
    "        \n",
    "    #df = analyze_results(save_dir, successful_functions, function_selector, neptune_logger=neptune_logger)\n",
    "    if neptune_logger: \n",
    "        neptune_logger.stop()\n",
    "    return successful_functions, save_dir, env, agent_policy\n",
    "\n",
    "\n",
    "def run_test_deephive(function_ids, iters, save_dir, model_path, config, **kwargs):\n",
    "    dimension = config[\"n_dim\"]\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "    successful_functions = []\n",
    "    for function_id in function_ids:\n",
    "        try:\n",
    "            config[\"function_id\"] = function_id\n",
    "            function_dim = function_selector.get_function(function_id)[\"dimension\"]\n",
    "            if config[\"n_dim\"] > function_dim:\n",
    "                print(f\"function {function_id} has {function_dim} dimensions, setting n_dim to {function_dim}\")\n",
    "                config[\"n_dim\"] = function_dim\n",
    "            else:\n",
    "                config[\"n_dim\"] = dimension\n",
    "            \n",
    "            \n",
    "            env, agent_policy = initialize(config, mode=\"test\", model_path=model_path)\n",
    "            _ = env.reset()[0]\n",
    "            agent_policy.load(model_path)\n",
    "            \n",
    "            try:\n",
    "                test_save_path = f\"{save_dir}/deephive/{function_id}\"\n",
    "                all_gbest_vals = test(env, agent_policy, iters, decay_start=10, decay_rate=0.9, \n",
    "                                min_action_std=0.0001, max_action_std=0.5, \n",
    "                                save_gif = False, save_path = test_save_path, function_id=function_id, neptune_logger=neptune_logger)\n",
    "                plot_individual_function_evaluation(all_gbest_vals, config[\"n_agents\"], f\"{test_save_path}/{function_id}_individual.png\", log_scale=config[\"log_scale\"])\n",
    "                num_function_evaluation(all_gbest_vals, config[\"n_agents\"], f\"{test_save_path}/{function_id}_num_evaluations.png\", log_scale=config[\"log_scale\"])\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[f\"deephive/{function_id}/individual\"].upload(f\"{test_save_path}/{function_id}_individual.png\")\n",
    "                    neptune_logger[f\"deephive/{function_id}/num_evaluations\"].upload(f\"{test_save_path}/{function_id}_num_evaluations.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Function {function_id} failed with error {e}\")\n",
    "            successful_functions.append(function_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Function {function_id} failed with error {e}\")\n",
    "        \n",
    "    #df = analyze_results(save_dir, successful_functions, function_selector, neptune_logger=neptune_logger)\n",
    "    if neptune_logger: \n",
    "        neptune_logger.stop()\n",
    "    return successful_functions, save_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_ids = [\"f01\"]#, \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\"]\n",
    "config[\"n_agents\"] = 40\n",
    "config[\"n_dim\"] = 30\n",
    "config['objective_function'] = \"BenchmarkFunctions\"\n",
    "config[\"ep_length\"] = 100\n",
    "config[\"log_scale\"] = True\n",
    "config[\"use_lbest\"] = True\n",
    "config[\"obs_dim\"] = 5\n",
    "MODEL_PATH = \"training_results/experiment_103/policy-4800.pth\"\n",
    "config[\"neighborhood_size\"] = 10\n",
    "env, agent_policy = initialize(config, mode=\"test\", model_path=MODEL_PATH)\n",
    "iters = 1\n",
    "save_dir = f\"new_test_results/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/\" \n",
    "title = \"experiment_103\"\n",
    "tags = \"testing with 40 agents and log scale\"\n",
    "# neptune_logger = None #initialize_logger(api_token, title, config, mode=\"test\")\n",
    "# successfull_functions, save_dir, env, policy = run_test(function_ids, iters, save_dir, MODEL_PATH, config, neptune_logger=neptune_logger)\n",
    "_ = env.reset()[0]\n",
    "env.compute_lbest()\n",
    "len(set(env.lbest_positions[:, 0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_experiments = [102, 103, 104]\n",
    "model_lists = [\"gbest\", \"lbest\", \"pbest\"]\n",
    "obs_dim = [4, 5, 4]\n",
    "function_ids = [\"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\",\n",
    "                \"f20\", \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \"f31\", \"f32\", \"f33\", \"f34\", \"f35\", \"f36\", \"f37\", \"f38\", \"f39\", \"f40\",\n",
    "                \"f41\", \"f42\", \"f43\", \"f44\", \"f45\", \"f46\", \"f47\", \"f48\", \"f49\", \"f50\"]\n",
    "#function_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "config[\"n_agents\"] = 40\n",
    "config[\"n_dim\"] = 30\n",
    "config['objective_function'] = \"BenchmarkFunctions\"\n",
    "config[\"ep_length\"] = 1000\n",
    "config[\"log_scale\"] = False\n",
    "iters = 10\n",
    "config[\"neighborhood_size\"] = 10\n",
    "\n",
    "base_save_dir = f\"new_test_results/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/\" \n",
    "for i, model in enumerate(model_experiments):\n",
    "    config[\"n_dim\"] = 30\n",
    "    config[\"n_agents\"] = 40\n",
    "    config[\"obs_dim\"] = obs_dim[i]\n",
    "    if i==1:\n",
    "        config[\"use_lbest\"] = True\n",
    "    else:\n",
    "        config[\"use_lbest\"] = False\n",
    "    MODEL_PATH = f\"training_results/experiment_{model}/policy-4800.pth\"\n",
    "    # env, agent_policy = initialize(config, mode=\"test\", model_path=MODEL_PATH)\n",
    "    title = f\"experiment_{model}\"\n",
    "    save_dir = base_save_dir + f\"model{model}/\"\n",
    "    tags = f\"testing with 40 agents and log scale\"\n",
    "    neptune_logger = None#initialize_logger(api_token, title, config, mode=\"test\")\n",
    "    successful_functions, save_dir = run_test_deephive(function_ids, iters, save_dir, MODEL_PATH, config, neptune_logger=neptune_logger)\n",
    "    if neptune_logger:\n",
    "        neptune_logger.stop()\n",
    "    print(f\"Experiment {model} completed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir =base_save_dir #f\"new_test_results/2024-03-06_22-07-01/\"\n",
    "model_paths = [save_dir + f\"model{model}/\" for model in [102, 103, 104]]\n",
    "model_lists = [\"gbest\", \"lbest\", \"pbest\"]\n",
    "df = analyze_results(save_dir, model_lists, model_paths, successful_functions, function_selector, neptune_logger=neptune_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{save_dir}/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSO-BSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSOBSA:\n",
    "    def __init__(self, fitness_function, dimension, swarm_size, inertia_weight, \n",
    "                 acc_coefficients, mix_rate, mutation_probability, neighborhood_size,\n",
    "                 lower_bound=-100, upper_bound=100, scale=False, apply_mutation=False):\n",
    "        self.scale_option = scale\n",
    "        self.num_evaluations = 0\n",
    "        self.lower_bound = lower_bound  \n",
    "        self.upper_bound = upper_bound\n",
    "        self.obj_function = fitness_function\n",
    "        self.dimension = dimension\n",
    "        self.apply_mutation = apply_mutation\n",
    "        self.swarm_size = swarm_size\n",
    "        self.inertia_weight = inertia_weight\n",
    "        self.acc_coefficients = acc_coefficients  # Tuple (c1, c2)\n",
    "        self.mix_rate = mix_rate\n",
    "        self.mutation_probability = mutation_probability\n",
    "        self.neighborhood_size = neighborhood_size\n",
    "        self.positions = np.random.uniform(low=lower_bound, high=upper_bound, size=(swarm_size, dimension))\n",
    "        if self.scale_option:\n",
    "            self.positions = self.scale(self.positions)\n",
    "        self.velocities = np.zeros((swarm_size, dimension))\n",
    "        self.fitness_values = self.fitness_function(self.positions)\n",
    "        self.pbest_positions = np.copy(self.positions)\n",
    "        self.pbest_values = np.copy(self.fitness_values)\n",
    "        self.gbest_position = self.pbest_positions[np.argmin(self.pbest_values)]\n",
    "        self.gbest_values = self.pbest_values.min()\n",
    "        self.gbest_history = []\n",
    "        self.gbest_history.append(self.gbest_values)\n",
    "        \n",
    "    def fitness_function(self, positions, no_tracking=False):\n",
    "        if not no_tracking:\n",
    "            self.num_evaluations += len(positions)\n",
    "        if self.scale_option:\n",
    "            positions = self.unscale(positions)\n",
    "        return self.obj_function(positions)\n",
    "        \n",
    "    def scale(self, positions):\n",
    "        return 2 * (positions - self.lower_bound) / (self.upper_bound - self.lower_bound) - 1\n",
    "    \n",
    "    def unscale(self, positions):\n",
    "        return ((positions + 1) / 2) * (self.upper_bound - self.lower_bound) + self.lower_bound\n",
    "    \n",
    "    def initialize_neighborhoods(self):\n",
    "        self.neighborhoods = []\n",
    "        for i in range(self.swarm_size):\n",
    "            neighborhood_indices = list(range(i - self.neighborhood_size, i)) + list(range(i + 1, i + 1 + self.neighborhood_size))\n",
    "            neighborhood_indices = [index % self.swarm_size for index in neighborhood_indices]\n",
    "            self.neighborhoods.append(neighborhood_indices)\n",
    "    \n",
    "    def compute_lbest(self):\n",
    "        self.lbest_positions = np.zeros((self.swarm_size, self.dimension))\n",
    "        for i in range(self.swarm_size):\n",
    "            neighborhood_fitnesses = self.fitness_values[self.neighborhoods[i]]\n",
    "            best_neighbor_idx = self.neighborhoods[i][np.argmin(neighborhood_fitnesses)]\n",
    "            self.lbest_positions[i] = self.positions[best_neighbor_idx]\n",
    "    \n",
    "    def update_velocity(self):\n",
    "        r1, r2 = np.random.rand(self.swarm_size, self.dimension), np.random.rand(self.swarm_size, self.dimension)\n",
    "        cognitive_component = self.acc_coefficients[0] * r1 * (self.pbest_positions - self.positions)\n",
    "        social_component = self.acc_coefficients[1] * r2 * (self.gbest_position - self.positions)\n",
    "        self.velocities = self.inertia_weight * self.velocities + cognitive_component + social_component\n",
    "    \n",
    "    def update_positions(self):\n",
    "        self.positions += self.velocities\n",
    "        if self.scale_option:\n",
    "            self.positions = np.clip(self.positions, -1, 1)  # Scaled bounds\n",
    "        else:\n",
    "            self.positions = np.clip(self.positions, self.lower_bound, self.upper_bound)\n",
    "    \n",
    "    def calculate_fitness(self):\n",
    "        self.fitness_values = self.fitness_function(self.positions)\n",
    "        self.update_pbest_and_gbest()\n",
    "        \n",
    "    def update_pbest_and_gbest(self):\n",
    "        better_mask = self.fitness_values < self.pbest_values\n",
    "        self.pbest_positions[better_mask] = self.positions[better_mask]\n",
    "        self.pbest_values[better_mask] = self.fitness_values[better_mask]\n",
    "        gbest_candidate_idx = np.argmin(self.pbest_values)\n",
    "        if self.pbest_values[gbest_candidate_idx] < self.gbest_values:\n",
    "            self.gbest_position = self.pbest_positions[gbest_candidate_idx]\n",
    "            self.gbest_values = self.pbest_values[gbest_candidate_idx]\n",
    "            \n",
    "    def mutate(self, positions, lbest_positions):\n",
    "        A = 2 * np.random.rand(self.swarm_size, self.dimension)\n",
    "        phi = np.random.rand(self.swarm_size, self.dimension)\n",
    "        return positions + A * (phi * lbest_positions - positions)\n",
    "    \n",
    "    def crossover(self, positions, trial_positions):\n",
    "        crossover_mask = np.random.rand(self.swarm_size, self.dimension) < self.mix_rate\n",
    "        return np.where(crossover_mask, trial_positions, positions)\n",
    "\n",
    "    def mutate_and_crossover(self):\n",
    "        for i in range(self.swarm_size):\n",
    "            if np.random.rand() < self.mutation_probability:\n",
    "                trial_positions = self.mutate(self.positions, self.lbest_positions[i])\n",
    "            else:\n",
    "                self.permute_lbest(i)\n",
    "                trial_positions = self.mutate(self.positions, self.lbest_positions[i])\n",
    "            new_positions = self.crossover(self.positions, trial_positions)\n",
    "            new_fitness = self.fitness_function(new_positions)\n",
    "            better_mask = new_fitness < self.fitness_values\n",
    "            self.positions[better_mask] = new_positions[better_mask]\n",
    "            self.fitness_values[better_mask] = new_fitness[better_mask]\n",
    "            self.update_pbest_and_gbest()\n",
    "            # if np.min(new_fitness) < self.fitness_function(self.gbest_position.reshape(1, -1)):\n",
    "            #     self.gbest_position = new_positions[np.argmin(new_fitness)]\n",
    "            \n",
    "    \n",
    "    def permute_lbest(self, particle_index):\n",
    "        neighbor_indices = self.neighborhoods[particle_index]\n",
    "        np.random.shuffle(neighbor_indices)\n",
    "        self.lbest_positions[particle_index] = self.positions[neighbor_indices[0]]\n",
    "    \n",
    "    def optimize(self, iterations):\n",
    "        self.initialize_neighborhoods()\n",
    "        for _ in range(iterations):\n",
    "            self.compute_lbest()\n",
    "            self.update_velocity()\n",
    "            self.update_positions()\n",
    "            self.calculate_fitness()\n",
    "            if self.apply_mutation:\n",
    "                self.mutate_and_crossover()\n",
    "            self.gbest_history.append(self.gbest_values)\n",
    "        return np.array(self.gbest_history)\n",
    "   \n",
    "function_id = \"f01\"\n",
    "function_info = function_selector.get_function(function_id)\n",
    "fitness_function = function_info[\"func\"]\n",
    "dimension = function_info[\"dimension\"]\n",
    "lower_bound = function_info[\"domain\"][0]\n",
    "upper_bound = function_info[\"domain\"][1]\n",
    "global_min = function_info[\"global_min\"] \n",
    "swarm_size = 40\n",
    "apply_mutation = True\n",
    "pso_bsa = PSOBSA(fitness_function=fitness_function,\n",
    "                dimension=dimension,\n",
    "                swarm_size=swarm_size,\n",
    "                inertia_weight=0.7,\n",
    "                acc_coefficients=(1.4, 1.4),\n",
    "                mix_rate=1,\n",
    "                mutation_probability=0.2,\n",
    "                neighborhood_size=10,\n",
    "                scale=False,\n",
    "                lower_bound=lower_bound,\n",
    "                upper_bound=upper_bound,\n",
    "                apply_mutation=apply_mutation)\n",
    "\n",
    "gbest_history = pso_bsa.optimize(iterations=100)\n",
    "print(gbest_history[-1])\n",
    "print(pso_bsa.num_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pso(function_id, iters, save_dir, ep_length, apply_mutation=False, swarm_size=40, **kwargs):\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "    try:\n",
    "        save_dir = f\"{save_dir}/{function_id}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        function_info = function_selector.get_function(function_id)\n",
    "        fitness_function = function_info[\"func\"]\n",
    "        dimension = kwargs.get(\"dimension\", function_info[\"dimension\"])\n",
    "        lower_bound = function_info[\"domain\"][0]\n",
    "        upper_bound = function_info[\"domain\"][1]\n",
    "        global_min = function_info[\"global_min\"]\n",
    "        all_gbest_vals = []\n",
    "        for iter in range(iters):\n",
    "            pso_bsa = PSOBSA(fitness_function=fitness_function,\n",
    "                dimension=dimension,\n",
    "                swarm_size=swarm_size,\n",
    "                inertia_weight=0.7,\n",
    "                acc_coefficients=(1.4, 1.4),\n",
    "                mix_rate=1,\n",
    "                mutation_probability=0.2,\n",
    "                neighborhood_size=10,\n",
    "                scale=False,\n",
    "                lower_bound=lower_bound,\n",
    "                upper_bound=upper_bound,\n",
    "                apply_mutation=apply_mutation)\n",
    "            gbest_history = pso_bsa.optimize(iterations=ep_length)\n",
    "            all_gbest_vals.append(gbest_history)\n",
    "        np.save(f\"{save_dir}/{function_id}_gbest_history.npy\", np.array(all_gbest_vals))\n",
    "        if neptune_logger:\n",
    "            neptune_logger[f\"pso/{function_id}/gbest_history\"].upload(f\"{save_dir}/{function_id}_gbest_history.npy\")\n",
    "    except Exception as e:\n",
    "        print(f\"Function {function_id} failed with error {e}\")\n",
    "    return np.array(all_gbest_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(function_ids, iters, save_dir, model_path, config, **kwargs):\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "    successfull_functions = []\n",
    "    for function_id in function_ids:\n",
    "        try:\n",
    "            config[\"function_id\"] = function_id\n",
    "            function_dim = function_selector.get_function(function_id)[\"dimension\"]\n",
    "            if config[\"n_dim\"] > function_dim:\n",
    "                print(f\"function {function_id} has {function_dim} dimensions, setting n_dim to {function_dim}\")\n",
    "                config[\"n_dim\"] = function_dim\n",
    "            env, agent_policy = initialize(config, mode=\"test\", model_path=model_path)\n",
    "            _ = env.reset()[0]\n",
    "            agent_policy.load(model_path)\n",
    "            \n",
    "            try:\n",
    "                test_save_path = f\"{save_dir}/deephive/{function_id}\"\n",
    "                all_gbest_vals = test(env, agent_policy, iters, decay_start=0, decay_rate=0.9, \n",
    "                                min_action_std=0.0001, max_action_std=0.5, \n",
    "                                save_gif = False, save_path = test_save_path, function_id=function_id, neptune_logger=neptune_logger)\n",
    "                plot_individual_function_evaluation(all_gbest_vals, config[\"n_agents\"], f\"{test_save_path}/{function_id}_individual.png\", log_scale=config[\"log_scale\"])\n",
    "                num_function_evaluation(all_gbest_vals, config[\"n_agents\"], f\"{test_save_path}/{function_id}_num_evaluations.png\", log_scale=config[\"log_scale\"])\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[f\"deephive/{function_id}/individual\"].upload(f\"{test_save_path}/{function_id}_individual.png\")\n",
    "                    neptune_logger[f\"deephive/{function_id}/num_evaluations\"].upload(f\"{test_save_path}/{function_id}_num_evaluations.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Function {function_id} failed with error {e}\")\n",
    "            try:\n",
    "                pso_save_path = f\"{save_dir}/pso/\"\n",
    "                apply_mutation = False\n",
    "                pso_gbest_history = run_pso(function_id=function_id, iters=iters, save_dir=pso_save_path, \n",
    "                                            ep_length=config[\"ep_length\"], apply_mutation=apply_mutation, \n",
    "                                            neptune_logger=neptune_logger, swarm_size=config[\"n_agents\"], dimension=config[\"n_dim\"])\n",
    "                print(f\"PSO gbest history: {pso_gbest_history.shape}\")\n",
    "                plot_individual_function_evaluation(pso_gbest_history, config[\"n_agents\"], f\"{pso_save_path}/{function_id}_individual.png\", log_scale=config[\"log_scale\"])\n",
    "                num_function_evaluation(pso_gbest_history, config[\"n_agents\"], f\"{pso_save_path}/{function_id}_num_evaluations.png\", log_scale=config[\"log_scale\"])\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[f\"pso/{function_id}/individual\"].upload(f\"{pso_save_path}/{function_id}_individual.png\")\n",
    "                    neptune_logger[f\"pso/{function_id}/num_evaluations\"].upload(f\"{pso_save_path}/{function_id}_num_evaluations.png\")\n",
    "            except Exception as e:\n",
    "                import traceback; traceback.print_exc();\n",
    "                print(f\"Function {function_id} failed with error {e}\")\n",
    "            try:\n",
    "                psobsa_save_path = f\"{save_dir}/psobsa/\"\n",
    "                apply_mutation = True\n",
    "                psobsa_gbest_history = run_pso(function_id=function_id, iters=iters, save_dir=psobsa_save_path, ep_length=config[\"ep_length\"],\n",
    "                                               apply_mutation=apply_mutation, neptune_logger=neptune_logger, swarm_size=config[\"n_agents\"], dimension=config[\"n_dim\"])\n",
    "                print(f\"PSO-BSA gbest history: {psobsa_gbest_history.shape}\")\n",
    "                plot_individual_function_evaluation(psobsa_gbest_history, config[\"n_agents\"], f\"{psobsa_save_path}/{function_id}_individual.png\", log_scale=config[\"log_scale\"])\n",
    "                num_function_evaluation(psobsa_gbest_history, config[\"n_agents\"], f\"{psobsa_save_path}/{function_id}_num_evaluations.png\", log_scale=config[\"log_scale\"])\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[f\"psobsa/{function_id}/individual\"].upload(f\"{psobsa_save_path}/{function_id}_individual.png\")\n",
    "                    neptune_logger[f\"psobsa/{function_id}/num_evaluations\"].upload(f\"{psobsa_save_path}/{function_id}_num_evaluations.png\")\n",
    "            except Exception as e:\n",
    "                import traceback; traceback.print_exc();    \n",
    "                print(f\"Function {function_id} failed with error {e}\")\n",
    "            successfull_functions.append(function_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Function {function_id} failed with error {e}\")\n",
    "        \n",
    "    df = analyze_results(save_dir, successfull_functions, function_selector, neptune_logger=neptune_logger)\n",
    "    if neptune_logger: \n",
    "        neptune_logger.stop()\n",
    "    return successfull_functions, save_dir, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_ids = [\"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\"]\n",
    "config[\"n_agents\"] = 40\n",
    "config[\"n_dim\"] = 30\n",
    "config['objective_function'] = \"BenchmarkFunctions\" \n",
    "config[\"ep_length\"] = 100 \n",
    "config[\"log_scale\"] = True\n",
    "\n",
    "MODEL_PATH = \"training_results/experiment_101/policy-4800.pth\"\n",
    "env, agent_policy = initialize(config, mode=\"test\", model_path=MODEL_PATH)\n",
    "iters = 1\n",
    "save_dir = f\"new_test_results/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/\" \n",
    "title = \"experiment_103\"\n",
    "tags = \"testing with 40 agents and log scale\"\n",
    "neptune_logger = None #initialize_logger(api_token, title, config, mode=\"test\")\n",
    "successfull_functions, save_dir, df = run_test(function_ids, iters, save_dir, MODEL_PATH, config, neptune_logger=neptune_logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results(save_dir, successfull_functions, function_selector, neptune_logger=neptune_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load(\"/Users/elotech/Desktop/DeepHiveV2/notebooks/new_test_results/2024-03-04_17-55-16/pso/f01/f01_gbest_history.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_results(base_path, successfull_functions, function_selector, **kwargs):\n",
    "#     neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "#     # Define a multi-level column structure: each optimizer has the same sub-columns\n",
    "#     columns = pd.MultiIndex.from_product([['deephive', 'pso', 'psobsa'], \n",
    "#                                           [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]],\n",
    "#                                          names=['optimizer', 'metric'])\n",
    "#     # Initialize an empty DataFrame with these columns\n",
    "#     df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "#     for function_id in successfull_functions:\n",
    "#         row_data = {}\n",
    "#         function_info = function_selector.get_function(function_id)\n",
    "#         function_opt_val = function_info[\"global_min\"]\n",
    "        \n",
    "#         # Loop through each optimizer\n",
    "#         for optimizer in ['deephive', 'pso', 'psobsa']:\n",
    "#             data_path = base_path + f\"{optimizer}/{function_id}/{function_id}_gbest_history.npy\"\n",
    "#             try:\n",
    "#                 # Attempt to load the optimizer's result and compute metrics\n",
    "#                 gbest_values = np.load(data_path) * -1\n",
    "#                 mean_val, lower_val, upper_val = mean_confidence_interval(gbest_values)\n",
    "#                 error_val = abs(mean_val[-1] - function_opt_val)\n",
    "#                 row_data[(optimizer, 'mean')] = mean_val[-1]\n",
    "#                 row_data[(optimizer, 'lower')] = lower_val[-1]\n",
    "#                 row_data[(optimizer, 'upper')] = upper_val[-1]\n",
    "#                 row_data[(optimizer, 'optimum')] = function_opt_val\n",
    "#                 row_data[(optimizer, 'error')] = error_val\n",
    "                \n",
    "#                 # Additional logging or plotting can be added here as needed\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {optimizer} for function {function_id}: {e}\")\n",
    "#                 # Fill missing values if any error occurs\n",
    "#                 for metric in [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]:\n",
    "#                     row_data[(optimizer, metric)] = np.nan\n",
    "        \n",
    "#         # After collecting data for all optimizers, add the row to the DataFrame\n",
    "#         #df = df.append(pd.Series(row_data, name=function_id))\n",
    "#         df.loc[function_id] = row_data\n",
    "    \n",
    "#     # Save the DataFrame to CSV\n",
    "#     df.to_csv(f\"{base_path}/results.csv\")\n",
    "    \n",
    "#     # Log the DataFrame if Neptune logger is provided\n",
    "#     if neptune_logger:\n",
    "#         neptune_logger[\"test/results\"].upload(f\"{base_path}/results.csv\")\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = f\"new_test_results/2024-03-04_17-31-05/\" \n",
    "# analyze_results(save_dir, [\"f01\", \"f02\"], function_selector, neptune_logger=neptune_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephive.environment.optimization_functions.benchmark_functions import FunctionSelector\n",
    "from deephive.environment.utils import mean_confidence_interval\n",
    "function_selector = FunctionSelector()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(base_path, model_lists, model_path_list, successful_functions, function_selector, **kwargs):\n",
    "    neptune_logger = kwargs.get(\"neptune_logger\", None)\n",
    "    # Define a multi-level column structure: each optimizer has the same sub-columns\n",
    "    columns = pd.MultiIndex.from_product([model_lists, \n",
    "                                          [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]],\n",
    "                                         names=['optimizer', 'metric'])\n",
    "    # Initialize an empty DataFrame with these columns\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for function_id in successful_functions:\n",
    "        row_data = {}\n",
    "        function_info = function_selector.get_function(function_id)\n",
    "        function_opt_val = function_info[\"global_min\"]\n",
    "        \n",
    "        # Loop through each optimizer\n",
    "        for optimizer, model_path in zip(model_lists, model_path_list):\n",
    "            data_path = model_path + f\"deephive/{function_id}/{function_id}_gbest_history.npy\"\n",
    "            try:\n",
    "                # Attempt to load the optimizer's result and compute metrics\n",
    "                gbest_values = np.load(data_path) * -1\n",
    "                mean_val, lower_val, upper_val = mean_confidence_interval(gbest_values)\n",
    "                error_val = abs(mean_val[-1] - function_opt_val)\n",
    "                row_data[(optimizer, 'mean')] = mean_val[-1]\n",
    "                row_data[(optimizer, 'lower')] = lower_val[-1]\n",
    "                row_data[(optimizer, 'upper')] = upper_val[-1]\n",
    "                row_data[(optimizer, 'optimum')] = function_opt_val\n",
    "                row_data[(optimizer, 'error')] = error_val\n",
    "                \n",
    "                # Additional logging or plotting can be added here as needed\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {optimizer} for function {function_id}: {e}\")\n",
    "                # Fill missing values if any error occurs\n",
    "                for metric in [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]:\n",
    "                    row_data[(optimizer, metric)] = np.nan\n",
    "        \n",
    "        # After collecting data for all optimizers, add the row to the DataFrame\n",
    "        #df = df.append(pd.Series(row_data, name=function_id))\n",
    "        df.loc[function_id] = row_data\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(f\"{base_path}/results.csv\")\n",
    "    \n",
    "    # Log the DataFrame if Neptune logger is provided\n",
    "    if neptune_logger:\n",
    "        neptune_logger[\"test/results\"].upload(f\"{base_path}/results.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_algo_path = \"other_algorithms/2024-03-10_10-38-15\"\n",
    "model_base_path = \"../new_test_results/2024-03-10_12-23-13\"\n",
    "\n",
    "columns = pd.MultiIndex.from_product([model_lists, \n",
    "                                          [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]],\n",
    "                                         names=['optimizer', 'metric'])\n",
    "# Initialize an empty DataFrame with these columns\n",
    "df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "function_ids = [\"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\",\n",
    "                \"f20\", \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \"f31\", \"f32\", \"f33\", \"f34\", \"f35\", \"f36\", \"f37\", \"f38\", \"f39\", \"f40\",\n",
    "                \"f41\", \"f42\", \"f43\", \"f44\", \"f45\", \"f46\", \"f47\", \"f48\", \"f49\", \"f50\"]\n",
    "\n",
    "other_algos = [\"DE\", \"GA\", \"PSO\", \"SA\"]\n",
    "deephive_algos = [\"model102\", \"model103\", \"model104\"]\n",
    "model_lists = other_algos + deephive_algos\n",
    "\n",
    "for function_id in function_ids:\n",
    "    row_data = {}\n",
    "    function_info = function_selector.get_function(function_id)\n",
    "    function_opt_val = function_info[\"global_min\"]\n",
    "    for model in model_lists:\n",
    "        if model in other_algos:\n",
    "            model_path = f\"{other_algo_path}/{function_id}/{model}_histories.npy\"\n",
    "        else:\n",
    "            model_path = f\"{model_base_path}/{model}/deephive/{function_id}/{function_id}_gbest_history.npy\"\n",
    "            \n",
    "        print(model_path)\n",
    "        try:\n",
    "            # Attempt to load the optimizer's result and compute metrics\n",
    "            gbest_values = np.load(model_path) * -1 if model in deephive_algos else np.load(model_path)\n",
    "            mean_val, lower_val, upper_val = mean_confidence_interval(gbest_values)\n",
    "            error_val = abs(mean_val[-1] - function_opt_val)\n",
    "            row_data[(model, 'mean')] = mean_val[-1]\n",
    "            row_data[(model, 'lower')] = lower_val[-1]\n",
    "            row_data[(model, 'upper')] = upper_val[-1]\n",
    "            row_data[(model, 'optimum')] = function_opt_val\n",
    "            row_data[(model, 'error')] = error_val\n",
    "            \n",
    "            # Additional logging or plotting can be added here as needed\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model} for function {function_id}: {e}\")\n",
    "            # Fill missing values if any error occurs\n",
    "            for metric in [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]:\n",
    "                row_data[(model, metric)] = np.nan\n",
    "                \n",
    "        df.loc[function_id] = row_data\n",
    "        \n",
    "df.to_excel(\"results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_algo_path = \"other_algorithms/2024-03-10_10-38-15\"\n",
    "model_base_path = \"../new_test_results/2024-03-10_12-23-13\"\n",
    "save_dir = \"optimization_results/\"\n",
    "\n",
    "columns = pd.MultiIndex.from_product([model_lists, \n",
    "                                          [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]],\n",
    "                                         names=['optimizer', 'metric'])\n",
    "# Initialize an empty DataFrame with these columns\n",
    "df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "function_ids = [\"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\",\n",
    "                \"f20\", \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \"f31\", \"f32\", \"f33\", \"f34\", \"f35\", \"f36\", \"f37\", \"f38\", \"f39\", \"f40\",\n",
    "                \"f41\", \"f42\", \"f43\", \"f44\", \"f45\", \"f46\", \"f47\", \"f48\", \"f49\", \"f50\"]\n",
    "\n",
    "other_algos = [\"DE\", \"GA\", \"PSO\", \"SA\"]\n",
    "deephive_algos = [\"model102\", \"model103\", \"model104\"]\n",
    "model_lists = other_algos + deephive_algos\n",
    "\n",
    "for function_id in function_ids:\n",
    "    row_data = {}\n",
    "    function_info = function_selector.get_function(function_id)\n",
    "    function_opt_val = function_info[\"global_min\"]\n",
    "    for model in model_lists:\n",
    "        if model in other_algos:\n",
    "            model_path = f\"{other_algo_path}/{function_id}/{model}_histories.npy\"\n",
    "        else:\n",
    "            model_path = f\"{model_base_path}/{model}/deephive/{function_id}/{function_id}_gbest_history.npy\"\n",
    "            \n",
    "        print(model_path)\n",
    "        try:\n",
    "            # Attempt to load the optimizer's result and compute metrics\n",
    "            gbest_values = np.load(model_path) * -1 if model in deephive_algos else np.load(model_path)\n",
    "            mean_val, lower_val, upper_val = mean_confidence_interval(gbest_values)\n",
    "            error_val = abs(mean_val[-1] - function_opt_val)\n",
    "            row_data[(model, 'mean')] = mean_val[-1]\n",
    "            row_data[(model, 'lower')] = lower_val[-1]\n",
    "            row_data[(model, 'upper')] = upper_val[-1]\n",
    "            row_data[(model, 'optimum')] = function_opt_val\n",
    "            row_data[(model, 'error')] = error_val\n",
    "            \n",
    "            # Additional logging or plotting can be added here as needed\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model} for function {function_id}: {e}\")\n",
    "            # Fill missing values if any error occurs\n",
    "            for metric in [\"mean\", \"lower\", \"upper\", \"optimum\", \"error\"]:\n",
    "                row_data[(model, metric)] = np.nan\n",
    "                \n",
    "        df.loc[function_id] = row_data\n",
    "        \n",
    "df.to_excel(f\"{save_dir}/results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing DE for function f01: \"None of [Index(['f01', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f01: \"None of [Index(['f01', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f01: \"None of [Index(['f01', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f01: \"None of [Index(['f01', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f01: \"None of [Index(['f01', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f01: \"None of [Index(['f01', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f01: \"None of [Index(['f01', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f02: \"None of [Index(['f02', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f02: \"None of [Index(['f02', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f02: \"None of [Index(['f02', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f02: \"None of [Index(['f02', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f02: \"None of [Index(['f02', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f02: \"None of [Index(['f02', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f02: \"None of [Index(['f02', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f03: \"None of [Index(['f03', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f03: \"None of [Index(['f03', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f03: \"None of [Index(['f03', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f03: \"None of [Index(['f03', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f03: \"None of [Index(['f03', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f03: \"None of [Index(['f03', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f03: \"None of [Index(['f03', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f04: \"None of [Index(['f04', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f04: \"None of [Index(['f04', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f04: \"None of [Index(['f04', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f04: \"None of [Index(['f04', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f04: \"None of [Index(['f04', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f04: \"None of [Index(['f04', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f04: \"None of [Index(['f04', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f05: \"None of [Index(['f05', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f05: \"None of [Index(['f05', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f05: \"None of [Index(['f05', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f05: \"None of [Index(['f05', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f05: \"None of [Index(['f05', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f05: \"None of [Index(['f05', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f05: \"None of [Index(['f05', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f06: \"None of [Index(['f06', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f06: \"None of [Index(['f06', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f06: \"None of [Index(['f06', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f06: \"None of [Index(['f06', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f06: \"None of [Index(['f06', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f06: \"None of [Index(['f06', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f06: \"None of [Index(['f06', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f07: \"None of [Index(['f07', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f07: \"None of [Index(['f07', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f07: \"None of [Index(['f07', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f07: \"None of [Index(['f07', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f07: \"None of [Index(['f07', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f07: \"None of [Index(['f07', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f07: \"None of [Index(['f07', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f08: \"None of [Index(['f08', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f08: \"None of [Index(['f08', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f08: \"None of [Index(['f08', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f08: \"None of [Index(['f08', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f08: \"None of [Index(['f08', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f08: \"None of [Index(['f08', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f08: \"None of [Index(['f08', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f09: \"None of [Index(['f09', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f09: \"None of [Index(['f09', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f09: \"None of [Index(['f09', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f09: \"None of [Index(['f09', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f09: \"None of [Index(['f09', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f09: \"None of [Index(['f09', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f09: \"None of [Index(['f09', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f10: \"None of [Index(['f10', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f10: \"None of [Index(['f10', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f10: \"None of [Index(['f10', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f10: \"None of [Index(['f10', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f10: \"None of [Index(['f10', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f10: \"None of [Index(['f10', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f10: \"None of [Index(['f10', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f11: \"None of [Index(['f11', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f11: \"None of [Index(['f11', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f11: \"None of [Index(['f11', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f11: \"None of [Index(['f11', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f11: \"None of [Index(['f11', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f11: \"None of [Index(['f11', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f11: \"None of [Index(['f11', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f12: \"None of [Index(['f12', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f12: \"None of [Index(['f12', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f12: \"None of [Index(['f12', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f12: \"None of [Index(['f12', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f12: \"None of [Index(['f12', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f12: \"None of [Index(['f12', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f12: \"None of [Index(['f12', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f13: \"None of [Index(['f13', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f13: \"None of [Index(['f13', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f13: \"None of [Index(['f13', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f13: \"None of [Index(['f13', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f13: \"None of [Index(['f13', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f13: \"None of [Index(['f13', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f13: \"None of [Index(['f13', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f14: \"None of [Index(['f14', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f14: \"None of [Index(['f14', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f14: \"None of [Index(['f14', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f14: \"None of [Index(['f14', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f14: \"None of [Index(['f14', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f14: \"None of [Index(['f14', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f14: \"None of [Index(['f14', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f15: \"None of [Index(['f15', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f15: \"None of [Index(['f15', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f15: \"None of [Index(['f15', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f15: \"None of [Index(['f15', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f15: \"None of [Index(['f15', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f15: \"None of [Index(['f15', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f15: \"None of [Index(['f15', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f16: \"None of [Index(['f16', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f16: \"None of [Index(['f16', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f16: \"None of [Index(['f16', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f16: \"None of [Index(['f16', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f16: \"None of [Index(['f16', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f16: \"None of [Index(['f16', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f16: \"None of [Index(['f16', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f17: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f17/DE_histories.npy'\n",
      "Error processing GA for function f17: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f17/GA_histories.npy'\n",
      "Error processing PSO for function f17: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f17/PSO_histories.npy'\n",
      "Error processing SA for function f17: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f17/SA_histories.npy'\n",
      "Error processing model102 for function f17: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model102/deephive/f17/f17_gbest_history.npy'\n",
      "Error processing model103 for function f17: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model103/deephive/f17/f17_gbest_history.npy'\n",
      "Error processing model104 for function f17: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model104/deephive/f17/f17_gbest_history.npy'\n",
      "Error processing DE for function f18: \"None of [Index(['f18', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f18: \"None of [Index(['f18', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f18: \"None of [Index(['f18', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f18: \"None of [Index(['f18', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f18: \"None of [Index(['f18', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f18: \"None of [Index(['f18', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f18: \"None of [Index(['f18', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f19: \"None of [Index(['f19', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f19: \"None of [Index(['f19', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f19: \"None of [Index(['f19', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f19: \"None of [Index(['f19', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f19: \"None of [Index(['f19', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f19: \"None of [Index(['f19', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f19: \"None of [Index(['f19', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f20: \"None of [Index(['f20', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f20: \"None of [Index(['f20', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f20: \"None of [Index(['f20', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f20: \"None of [Index(['f20', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f20: \"None of [Index(['f20', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f20: \"None of [Index(['f20', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f20: \"None of [Index(['f20', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f21: \"None of [Index(['f21', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f21: \"None of [Index(['f21', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f21: \"None of [Index(['f21', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f21: \"None of [Index(['f21', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f21: \"None of [Index(['f21', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f21: \"None of [Index(['f21', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f21: \"None of [Index(['f21', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f22: \"None of [Index(['f22', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f22: \"None of [Index(['f22', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f22: \"None of [Index(['f22', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f22: \"None of [Index(['f22', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f22: \"None of [Index(['f22', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f22: \"None of [Index(['f22', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f22: \"None of [Index(['f22', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f23: \"None of [Index(['f23', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f23: \"None of [Index(['f23', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f23: \"None of [Index(['f23', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f23: \"None of [Index(['f23', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f23: \"None of [Index(['f23', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f23: \"None of [Index(['f23', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f23: \"None of [Index(['f23', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f24: \"None of [Index(['f24', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f24: \"None of [Index(['f24', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f24: \"None of [Index(['f24', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f24: \"None of [Index(['f24', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f24: \"None of [Index(['f24', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f24: \"None of [Index(['f24', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f24: \"None of [Index(['f24', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f25: \"None of [Index(['f25', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f25: \"None of [Index(['f25', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f25: \"None of [Index(['f25', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f25: \"None of [Index(['f25', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f25: \"None of [Index(['f25', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f25: \"None of [Index(['f25', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f25: \"None of [Index(['f25', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f26: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f26/DE_histories.npy'\n",
      "Error processing GA for function f26: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f26/GA_histories.npy'\n",
      "Error processing PSO for function f26: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f26/PSO_histories.npy'\n",
      "Error processing SA for function f26: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f26/SA_histories.npy'\n",
      "Error processing model102 for function f26: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model102/deephive/f26/f26_gbest_history.npy'\n",
      "Error processing model103 for function f26: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model103/deephive/f26/f26_gbest_history.npy'\n",
      "Error processing model104 for function f26: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model104/deephive/f26/f26_gbest_history.npy'\n",
      "Error processing DE for function f27: \"None of [Index(['f27', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f27: \"None of [Index(['f27', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f27: \"None of [Index(['f27', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f27: \"None of [Index(['f27', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f27: \"None of [Index(['f27', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f27: \"None of [Index(['f27', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f27: \"None of [Index(['f27', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f28: \"None of [Index(['f28', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f28: \"None of [Index(['f28', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f28: \"None of [Index(['f28', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f28: \"None of [Index(['f28', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f28: \"None of [Index(['f28', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f28: \"None of [Index(['f28', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f28: \"None of [Index(['f28', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f29: \"None of [Index(['f29', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f29: \"None of [Index(['f29', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f29: \"None of [Index(['f29', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f29: \"None of [Index(['f29', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f29: \"None of [Index(['f29', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f29: \"None of [Index(['f29', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f29: \"None of [Index(['f29', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f30: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f30/DE_histories.npy'\n",
      "Error processing GA for function f30: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f30/GA_histories.npy'\n",
      "Error processing PSO for function f30: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f30/PSO_histories.npy'\n",
      "Error processing SA for function f30: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f30/SA_histories.npy'\n",
      "Error processing model102 for function f30: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model102/deephive/f30/f30_gbest_history.npy'\n",
      "Error processing model103 for function f30: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model103/deephive/f30/f30_gbest_history.npy'\n",
      "Error processing model104 for function f30: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model104/deephive/f30/f30_gbest_history.npy'\n",
      "Error processing DE for function f31: \"None of [Index(['f31', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f31: \"None of [Index(['f31', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f31: \"None of [Index(['f31', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f31: \"None of [Index(['f31', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f31: \"None of [Index(['f31', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f31: \"None of [Index(['f31', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f31: \"None of [Index(['f31', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f32: \"None of [Index(['f32', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f32: \"None of [Index(['f32', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f32: \"None of [Index(['f32', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f32: \"None of [Index(['f32', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f32: \"None of [Index(['f32', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f32: \"None of [Index(['f32', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f32: \"None of [Index(['f32', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f33: \"None of [Index(['f33', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f33: \"None of [Index(['f33', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f33: \"None of [Index(['f33', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f33: \"None of [Index(['f33', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f33: \"None of [Index(['f33', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f33: \"None of [Index(['f33', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f33: \"None of [Index(['f33', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f34: \"None of [Index(['f34', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f34: \"None of [Index(['f34', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f34: \"None of [Index(['f34', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f34: \"None of [Index(['f34', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f34: \"None of [Index(['f34', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f34: \"None of [Index(['f34', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f34: \"None of [Index(['f34', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f35: \"None of [Index(['f35', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f35: \"None of [Index(['f35', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f35: \"None of [Index(['f35', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f35: \"None of [Index(['f35', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f35: \"None of [Index(['f35', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f35: \"None of [Index(['f35', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f35: \"None of [Index(['f35', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f36: \"None of [Index(['f36', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f36: \"None of [Index(['f36', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f36: \"None of [Index(['f36', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f36: \"None of [Index(['f36', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f36: \"None of [Index(['f36', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f36: \"None of [Index(['f36', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f36: \"None of [Index(['f36', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f37: \"None of [Index(['f37', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f37: \"None of [Index(['f37', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f37: \"None of [Index(['f37', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f37: \"None of [Index(['f37', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f37: \"None of [Index(['f37', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f37: \"None of [Index(['f37', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f37: \"None of [Index(['f37', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f38: \"None of [Index(['f38', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f38: \"None of [Index(['f38', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f38: \"None of [Index(['f38', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f38: \"None of [Index(['f38', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f38: \"None of [Index(['f38', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f38: \"None of [Index(['f38', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f38: \"None of [Index(['f38', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f39: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f39/DE_histories.npy'\n",
      "Error processing GA for function f39: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f39/GA_histories.npy'\n",
      "Error processing PSO for function f39: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f39/PSO_histories.npy'\n",
      "Error processing SA for function f39: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f39/SA_histories.npy'\n",
      "Error processing model102 for function f39: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model102/deephive/f39/f39_gbest_history.npy'\n",
      "Error processing model103 for function f39: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model103/deephive/f39/f39_gbest_history.npy'\n",
      "Error processing model104 for function f39: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model104/deephive/f39/f39_gbest_history.npy'\n",
      "Error processing DE for function f40: \"None of [Index(['f40', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f40: \"None of [Index(['f40', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f40: \"None of [Index(['f40', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f40: \"None of [Index(['f40', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f40: \"None of [Index(['f40', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f40: \"None of [Index(['f40', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f40: \"None of [Index(['f40', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f41: \"None of [Index(['f41', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f41: \"None of [Index(['f41', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f41: \"None of [Index(['f41', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f41: \"None of [Index(['f41', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f41: \"None of [Index(['f41', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f41: \"None of [Index(['f41', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f41: \"None of [Index(['f41', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f42: \"None of [Index(['f42', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f42: \"None of [Index(['f42', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f42: \"None of [Index(['f42', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f42: \"None of [Index(['f42', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f42: \"None of [Index(['f42', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f42: \"None of [Index(['f42', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f42: \"None of [Index(['f42', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f43: \"None of [Index(['f43', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f43: \"None of [Index(['f43', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f43: \"None of [Index(['f43', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f43: \"None of [Index(['f43', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f43: \"None of [Index(['f43', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f43: \"None of [Index(['f43', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f43: \"None of [Index(['f43', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f44: \"None of [Index(['f44', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f44: \"None of [Index(['f44', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f44: \"None of [Index(['f44', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f44: \"None of [Index(['f44', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f44: \"None of [Index(['f44', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f44: \"None of [Index(['f44', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f44: \"None of [Index(['f44', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f45: \"None of [Index(['f45', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f45: \"None of [Index(['f45', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f45: \"None of [Index(['f45', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f45: \"None of [Index(['f45', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f45: \"None of [Index(['f45', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f45: \"None of [Index(['f45', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f45: \"None of [Index(['f45', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f46: \"None of [Index(['f46', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f46: \"None of [Index(['f46', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f46: \"None of [Index(['f46', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f46: \"None of [Index(['f46', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f46: \"None of [Index(['f46', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f46: \"None of [Index(['f46', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f46: \"None of [Index(['f46', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f47: \"None of [Index(['f47', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f47: \"None of [Index(['f47', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f47: \"None of [Index(['f47', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f47: \"None of [Index(['f47', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f47: \"None of [Index(['f47', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f47: \"None of [Index(['f47', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f47: \"None of [Index(['f47', 0], dtype='object')] are in the [index]\"\n",
      "Error processing DE for function f48: \"None of [Index(['f48', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f48: \"None of [Index(['f48', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f48: \"None of [Index(['f48', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f48: \"None of [Index(['f48', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f48: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model102/deephive/f48/f48_gbest_history.npy'\n",
      "Error processing model103 for function f48: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model103/deephive/f48/f48_gbest_history.npy'\n",
      "Error processing model104 for function f48: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model104/deephive/f48/f48_gbest_history.npy'\n",
      "Error processing DE for function f49: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f49/DE_histories.npy'\n",
      "Error processing GA for function f49: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f49/GA_histories.npy'\n",
      "Error processing PSO for function f49: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f49/PSO_histories.npy'\n",
      "Error processing SA for function f49: [Errno 2] No such file or directory: 'other_algorithms/2024-03-10_10-38-15/f49/SA_histories.npy'\n",
      "Error processing model102 for function f49: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model102/deephive/f49/f49_gbest_history.npy'\n",
      "Error processing model103 for function f49: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model103/deephive/f49/f49_gbest_history.npy'\n",
      "Error processing model104 for function f49: [Errno 2] No such file or directory: '../new_test_results/2024-03-10_12-23-13/model104/deephive/f49/f49_gbest_history.npy'\n",
      "Error processing DE for function f50: \"None of [Index(['f50', 0], dtype='object')] are in the [index]\"\n",
      "Error processing GA for function f50: \"None of [Index(['f50', 0], dtype='object')] are in the [index]\"\n",
      "Error processing PSO for function f50: \"None of [Index(['f50', 0], dtype='object')] are in the [index]\"\n",
      "Error processing SA for function f50: \"None of [Index(['f50', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model102 for function f50: \"None of [Index(['f50', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model103 for function f50: \"None of [Index(['f50', 0], dtype='object')] are in the [index]\"\n",
      "Error processing model104 for function f50: \"None of [Index(['f50', 0], dtype='object')] are in the [index]\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Save the DataFrame\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/iteration_wise_results.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/mlEnv/lib/python3.11/site-packages/pandas/core/generic.py:2252\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options)\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2241\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2242\u001b[0m     df,\n\u001b[1;32m   2243\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2250\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[1;32m   2251\u001b[0m )\n\u001b[0;32m-> 2252\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/mlEnv/lib/python3.11/site-packages/pandas/io/formats/excel.py:940\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[1;32m    937\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 940\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_cells\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatted_cells\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m need_save:\n",
      "File \u001b[0;32m~/.virtualenvs/mlEnv/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py:485\u001b[0m, in \u001b[0;36mOpenpyxlWriter._write_cells\u001b[0;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[1;32m    480\u001b[0m     freeze_panes \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m], freeze_panes)\n\u001b[1;32m    481\u001b[0m     wks\u001b[38;5;241m.\u001b[39mfreeze_panes \u001b[38;5;241m=\u001b[39m wks\u001b[38;5;241m.\u001b[39mcell(\n\u001b[1;32m    482\u001b[0m         row\u001b[38;5;241m=\u001b[39mfreeze_panes[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, column\u001b[38;5;241m=\u001b[39mfreeze_panes[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m cells:\n\u001b[1;32m    486\u001b[0m     xcell \u001b[38;5;241m=\u001b[39m wks\u001b[38;5;241m.\u001b[39mcell(\n\u001b[1;32m    487\u001b[0m         row\u001b[38;5;241m=\u001b[39mstartrow \u001b[38;5;241m+\u001b[39m cell\u001b[38;5;241m.\u001b[39mrow \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, column\u001b[38;5;241m=\u001b[39mstartcol \u001b[38;5;241m+\u001b[39m cell\u001b[38;5;241m.\u001b[39mcol \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    489\u001b[0m     xcell\u001b[38;5;241m.\u001b[39mvalue, fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_with_fmt(cell\u001b[38;5;241m.\u001b[39mval)\n",
      "File \u001b[0;32m~/.virtualenvs/mlEnv/lib/python3.11/site-packages/pandas/io/formats/excel.py:883\u001b[0m, in \u001b[0;36mExcelFormatter.get_formatted_cells\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_formatted_cells\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[ExcelCell]:\n\u001b[0;32m--> 883\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_header(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_body()):\n\u001b[1;32m    884\u001b[0m         cell\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_value(cell\u001b[38;5;241m.\u001b[39mval)\n\u001b[1;32m    885\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cell\n",
      "File \u001b[0;32m~/.virtualenvs/mlEnv/lib/python3.11/site-packages/pandas/io/formats/excel.py:628\u001b[0m, in \u001b[0;36mExcelFormatter._format_header_mi\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m lnum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m--> 628\u001b[0m     coloffset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_cells:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# Format multi-index as a merged cells.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lnum, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(columns\u001b[38;5;241m.\u001b[39mnames):\n",
      "File \u001b[0;32m~/.virtualenvs/mlEnv/lib/python3.11/site-packages/pandas/core/indexes/multi.py:2012\u001b[0m, in \u001b[0;36mMultiIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2010\u001b[0m retval \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lev, level_codes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodes):\n\u001b[0;32m-> 2012\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlevel_codes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2013\u001b[0m         retval\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your paths and lists setup\n",
    "other_algo_path = \"other_algorithms/2024-03-10_10-38-15\"\n",
    "model_base_path = \"../new_test_results/2024-03-10_12-23-13\"\n",
    "save_dir = \"optimization_results/\"\n",
    "\n",
    "other_algos = [\"DE\", \"GA\", \"PSO\", \"SA\"]\n",
    "deephive_algos = [\"model102\", \"model103\", \"model104\"]\n",
    "model_lists = other_algos + deephive_algos\n",
    "\n",
    "function_ids = [f\"f0{i}\" if i < 10 else f\"f{i}\" for i in range(1, 51)]\n",
    "\n",
    "# Prepare the DataFrame with specific columns\n",
    "columns = pd.MultiIndex.from_product([model_lists, [\"mean\", \"lowest\"]], names=['optimizer', 'metric'])\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for function_id in function_ids:\n",
    "    for model in model_lists:\n",
    "        try:\n",
    "            if model in other_algos:\n",
    "                model_path = f\"{other_algo_path}/{function_id}/{model}_histories.npy\"\n",
    "            else:\n",
    "                model_path = f\"{model_base_path}/{model}/deephive/{function_id}/{function_id}_gbest_history.npy\"\n",
    "            \n",
    "            gbest_values = np.load(model_path)\n",
    "            if model in deephive_algos:\n",
    "                gbest_values *= -1  # Adjust if necessary\n",
    "            \n",
    "            for iteration in range(gbest_values.shape[0]):\n",
    "                mean_val = np.mean(gbest_values[iteration])\n",
    "                lowest_val = np.min(gbest_values[iteration])\n",
    "                df.at[(function_id, iteration), (model, 'mean')] = mean_val\n",
    "                df.at[(function_id, iteration), (model, 'lowest')] = lowest_val\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model} for function {function_id}: {e}\")\n",
    "\n",
    "# Reindex the DataFrame for better readability\n",
    "df.index = pd.MultiIndex.from_tuples(df.index, names=['function_id', 'iteration'])\n",
    "\n",
    "# Ensure the directory exists or create it before saving\n",
    "import os\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame\n",
    "df.to_excel(f\"{save_dir}/iteration_wise_results.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
