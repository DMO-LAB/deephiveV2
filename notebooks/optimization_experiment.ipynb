{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import os\n",
    "from environment.deephive_utils import *\n",
    "from environment.utils import *\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent_policy, timesteps, iters, save_gif=False, result_path=\"experiment/\", save_interval=10,\n",
    "                   split_agents=True):\n",
    "    gbest_values = []\n",
    "    if save_gif:\n",
    "        os.makedirs(result_path, exist_ok=True)\n",
    "    for iter in range(iters):\n",
    "        #print(\"Iteration: \", iter)\n",
    "        observation_info = env.reset()\n",
    "        episode_gbVals = []\n",
    "        for _ in range(timesteps):\n",
    "            episode_gbVals.append(env.gbest[-1])\n",
    "            exploiters_action =  get_action(observation_info, agent_policy, env)\n",
    "            explorer_action = get_informed_action(env)\n",
    "            # split the agents into two groups and let one group exploit and the other explore\n",
    "            actions = np.zeros((env.n_agents, env.n_dim))\n",
    "            if split_agents:\n",
    "                actions[:env.n_agents//2] = exploiters_action[:env.n_agents//2]\n",
    "                actions[env.n_agents//2:] = explorer_action[env.n_agents//2:]\n",
    "            else:\n",
    "                actions = exploiters_action\n",
    "            observation_info, reward, done, info = env.step(actions)\n",
    "        gbest_values.append(episode_gbVals)\n",
    "        if save_gif and iter % save_interval == 0:\n",
    "            _ = env.render(type=\"history\", file_path=result_path + \"iter_\" + str(iter) + \".gif\")\n",
    "    return gbest_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded policy from:  ../models/exploiting_model.pth\n",
      "Loaded policy from:  ../models/policy-50000.pth\n"
     ]
    }
   ],
   "source": [
    "# #Experiment 1: VARIANCE DRIVEN EXPLORERS AND EXPLOITERS\n",
    "config_path = '../config/config.json'\n",
    "model_path = \"../models/exploiting_model.pth\"\n",
    "mode = \"test\"\n",
    "env1, agent_policy1 = initialize(config_path, mode=mode, model_path=model_path)\n",
    "config = parse_config(config_path)\n",
    "#agent_policy1.set_action_std(config[\"test_action_std\"])\n",
    "\n",
    "#Experiment 2: NO DIVISION\n",
    "model_path = \"../models/policy-50000.pth\"\n",
    "mode = \"test\"\n",
    "env2, agent_policy2 = initialize(config_path, mode=mode, model_path=model_path)\n",
    "config = parse_config(config_path)\n",
    "agent_policy2.set_action_std(config[\"test_action_std\"])\n",
    "\n",
    "iters = 10\n",
    "exp_num = 1\n",
    "timesteps = 20\n",
    "experiments = [\n",
    "    [env1, agent_policy1, f\"variance_driven_exploration_{exp_num}\", timesteps, iters, True, \"experiment/\", True],\n",
    "    [env2, agent_policy2, f\"no_division_{exp_num}\", timesteps, iters, True, \"experiment/\", False]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symbols = [\"-\", \"--\", \"-.\", \":\"]\n",
    "all_colors = [\"r\", \"g\", \"b\", \"k\"]\n",
    "\n",
    "symbol_list = []\n",
    "color_list = []\n",
    "label_list = []\n",
    "gbest_values = []\n",
    "for i, experiment in enumerate(experiments):\n",
    "    env, agent_policy, name, timesteps, iters, save_gif, result_path, split_agent = experiment\n",
    "    gbest_value = run_experiment(env, agent_policy, timesteps, iters, save_gif, f\"{result_path}{name}/\", split_agents=split_agent, save_interval=1)\n",
    "    env.surrogate.plot_surrogate(save_dir=f\"{result_path}{name}/surrogate.png\")\n",
    "    env.surrogate.plot_variance(save_dir=f\"{result_path}{name}/variance.png\")\n",
    "    np.save(result_path + name + \"_gbest_values.npy\", gbest_values)\n",
    "    gbest_values.append(gbest_value)\n",
    "    symbol_list.append(all_symbols[i])\n",
    "    color_list.append(all_colors[i])\n",
    "    label_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of function evaluations: 10\n",
      "Number of algorithms: 2\n"
     ]
    }
   ],
   "source": [
    "plot_num_function_evaluation(fopt=np.array(gbest_values), label_list=label_list, symbol_list=symbol_list, color_list=color_list, save_dir=f\"experiment/comparison_{exp_num}.png\",\n",
    "                             n_agents=env.n_agents, opt_value=env.objective_function.optimal_value(env.n_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import str2bool\n",
    "\n",
    "str2bool.str2bool(\"False\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
