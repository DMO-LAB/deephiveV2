{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from policies.mappo import MAPPO\n",
    "from environment.optimization_environment import OptimizationEnv\n",
    "from environment.utils import parse_config\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import neptune\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from other_algorithms.pso import ParticleSwarmOptimizer\n",
    "import time\n",
    "from scipy.stats import multivariate_normal\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(config_path, mode=\"train\", **kwargs):\n",
    "    env = OptimizationEnv(config_path)\n",
    "    agent_policy = MAPPO(config_path)\n",
    "    if mode == \"test\" or mode == \"benchmark\":\n",
    "        model_path = kwargs.get(\"model_path\", None)\n",
    "        if model_path is None:\n",
    "            raise ValueError(\"Model path must be provided for testing\")\n",
    "        agent_policy.load(model_path)\n",
    "    return env, agent_policy\n",
    "\n",
    "def print_items(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key, value)\n",
    "        \n",
    "def get_action(observation_info, agent_policy, env):\n",
    "    observation, observation_std = observation_info\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    for dim in range(env.n_dim):\n",
    "        observation[dim] = observation[dim].astype(np.float32)\n",
    "        #print(observation[dim])\n",
    "        observation_std[dim] = observation_std[dim].astype(np.float32)\n",
    "        action = agent_policy.select_action(observation[dim], observation_std[dim])\n",
    "        actions[:, dim] = action\n",
    "    return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../config/config.json'\n",
    "model_path = \"../models/exploiting_model.pth\"\n",
    "mode = \"test\"\n",
    "env, agent_policy = initialize(config_path, mode=mode, model_path=model_path)\n",
    "config = parse_config(config_path)\n",
    "agent_policy.set_action_std(config[\"test_action_std\"])\n",
    "\n",
    "def display_gmm_details(gmm):\n",
    "    print(\"Means: \", gmm.means_)\n",
    "    print(\"Covariances: \", gmm.covariances_)\n",
    "    print(\"Weights: \", gmm.weights_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_checkpoints_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_informed_action(env):\n",
    "    # let the action be the distance it takes for the agents to get to the a random point in the high std points\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    high_std_points, high_std_points_std = env._get_unexplored_area()\n",
    "    taken_points_index = []\n",
    "    for agents in range(env.n_agents):\n",
    "        while True:\n",
    "            index = np.random.randint(0, high_std_points.shape[0])\n",
    "            if index not in taken_points_index:\n",
    "                taken_points_index.append(index)\n",
    "                break\n",
    "        agent_target = high_std_points[index]\n",
    "        # get the difference between the agent's current position and the target\n",
    "        diff = agent_target - env.state[agents][:env.n_dim]\n",
    "        # add noise to the action\n",
    "        actions[agents] = diff #+ np.random.normal(-0.1, 0.1, size=env.n_dim)\n",
    "\n",
    "    return actions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_informed_action(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIANCE DRIVEN EXPLORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_info = env.reset()\n",
    "rewards = []\n",
    "std_comp = []\n",
    "hig_std_comp = []\n",
    "iters = 11\n",
    "results_buffer = np.zeros((iters, 4))\n",
    "\n",
    "\n",
    "for i in range(iters):\n",
    "    print(\"Iteration: \", i)\n",
    "    #action =  get_action(observation_info, agent_policy, env)\n",
    "    action = get_informed_action(env)\n",
    "    observation_info, reward, done, info = env.step(action)\n",
    "    rewards.append(np.mean(reward)) \n",
    "    env.surrogate.plot_checkpoints_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_checkpoints_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(type=\"history\", file_path=\"new_history1.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLOITERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_info = env.reset()\n",
    "rewards = []\n",
    "std_comp = []\n",
    "hig_std_comp = []\n",
    "iters = 11\n",
    "results_buffer = np.zeros((iters, 4))\n",
    "\n",
    "\n",
    "for i in range(iters):\n",
    "    print(\"Iteration: \", i)\n",
    "    action =  get_action(observation_info, agent_policy, env)\n",
    "    #action = get_informed_action(env)\n",
    "    observation_info, reward, done, info = env.step(action)\n",
    "    #env.surrogate.plot_checkpoints_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_checkpoints_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_surrogate()\n",
    "env.surrogate.plot_variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINING EXPLORERS AND EXPLOITERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_info = env.reset()\n",
    "rewards = []\n",
    "std_comp = []\n",
    "hig_std_comp = []\n",
    "iters = 11\n",
    "results_buffer = np.zeros((iters, 4))\n",
    "\n",
    "\n",
    "for i in range(iters):\n",
    "    print(\"Iteration: \", i)\n",
    "    exploiters_action =  get_action(observation_info, agent_policy, env)\n",
    "    explorer_action = get_informed_action(env)\n",
    "    # split the agents into two groups and let one group exploit and the other explore\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    actions[:env.n_agents//2] = exploiters_action[:env.n_agents//2]\n",
    "    actions[env.n_agents//2:] = explorer_action[env.n_agents//2:]\n",
    "    #print(actions)\n",
    "    observation_info, reward, done, info = env.step(actions)\n",
    "    env.surrogate.plot_checkpoints_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_checkpoints_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_surrogate()\n",
    "env.surrogate.plot_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.gp.kernel_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(type=\"history\", file_path=\"new_history2.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.deephive2 import num_function_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_info = env.reset()\n",
    "rewards = []\n",
    "std_comp = []\n",
    "hig_std_comp = []\n",
    "iters = 20\n",
    "results_buffer = np.zeros((iters, 4))\n",
    "\n",
    "global_best_value = []\n",
    "global_best_value.append(env.gbest[-1])\n",
    "for i in range(iters):\n",
    "    print(\"Iteration: \", i)\n",
    "    exploiters_action =  get_action(observation_info, agent_policy, env)\n",
    "    explorer_action = get_informed_action(env)\n",
    "    # split the agents into two groups and let one group exploit and the other explore\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    actions[:env.n_agents//2] = exploiters_action[:env.n_agents//2]\n",
    "    actions[env.n_agents//2:] = explorer_action[env.n_agents//2:]\n",
    "    #print(actions)\n",
    "    observation_info, reward, done, info = env.step(actions)\n",
    "    global_best_value.append(env.gbest[-1])\n",
    "\n",
    "    env.surrogate.plot_surrogate()\n",
    "        #env.surrogate.plot_checkpoints_state()\n",
    "    # env.surrogate.plot_checkpoints_state()\n",
    "# save_dir = \"results\"\n",
    "# num_function_evaluation(global_best_value, env.n_agents, save_dir, env.objective_function.optimal_value(env.n_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(type=\"history\", file_path=\"new_history4.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exploration.gp_surrogate import GPSurrogateModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = env.bounds\n",
    "resolution = 0.01\n",
    "\n",
    "x_min, y_min = bounds[0]\n",
    "x_max, y_max = bounds[1]\n",
    "\n",
    "\n",
    "# Create a meshgrid of x and y values within the bounds\n",
    "x_values = np.arange(x_min, x_max + resolution, resolution)\n",
    "y_values = np.arange(y_min, y_max + resolution, resolution)\n",
    "xx, yy = np.meshgrid(x_values, y_values)\n",
    "\n",
    "# Flatten the meshgrid to get the individual x and y coordinates\n",
    "x_coordinates = xx.flatten()\n",
    "y_coordinates = yy.flatten()\n",
    "\n",
    "# Create a list of points as (x, y) tuples\n",
    "points = list(zip(x_coordinates, y_coordinates))\n",
    "\n",
    "# Convert the list of points to a NumPy array\n",
    "points_array = np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sample = points_array\n",
    "initial_values = env.objective_function.evaluate(initial_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp = GPSurrogateModule(initial_samples=initial_sample, initial_values=initial_values, bounds=bounds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
