{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephive.optimizers import deephive as dh \n",
    "from deephive.environment.utils import * \n",
    "from datetime import datetime\n",
    "import os \n",
    "import neptune \n",
    "from deephive.environment.deephive_utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/exp_config.json\"\n",
    "model_path = \"../models/pbest_unfreeze.pth\"\n",
    "model_path_2 = \"../models/gbest.pth\"\n",
    "\n",
    "def initialize_logger(api_token, tags, config, mode=\"train\"):\n",
    "    run = neptune.init_run(\n",
    "    project=\"DMO-LAB/DeepHive-V2\",\n",
    "    source_files=[\"environment\", \"policies\", \"deephive.py\", \"config\"],\n",
    "    api_token=api_token,\n",
    "    tags=[tags, mode, config[\"objective_function\"], str(config[\"layer_size\"])]\n",
    "    )\n",
    "    neptune_logger = run\n",
    "    return neptune_logger\n",
    "\n",
    "def get_action(observation_info, agent_policy, env, observation_std=None, random=False):\n",
    "    # Ensure observation_info is a numpy array\n",
    "    \n",
    "    if random:\n",
    "        print(\"Random action\")\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    observation = observation_info\n",
    "    if not isinstance(observation, np.ndarray):\n",
    "        observation = np.array(observation)\n",
    "        assert observation.shape[0] == env.n_dim, \"Observation must have the same number of dimensions as the environment\"\n",
    "\n",
    "    # Initialize observation_std with zeros or use provided std, ensuring it matches the shape of observation\n",
    "    if observation_std is None:\n",
    "        observation_std = np.zeros_like(observation)\n",
    "    else:\n",
    "        observation_std = np.array(observation_std)\n",
    "\n",
    "    # Convert observations and stds to the appropriate format (Flatten if necessary)\n",
    "    # Assuming observation and observation_std are 2D arrays of shape (n_agents, n_dim)\n",
    "    # Flatten observation and observation_std for processing\n",
    "    observation_flat = observation.reshape(env.n_dim * env.n_agents, -1)  # Flatten to 1D array\n",
    "    observation_std_flat = observation_std.reshape(-1)  # Flatten to 1D array\n",
    "    # Pass the entire flattened observation and std arrays to select_action\n",
    "    action_flat = agent_policy.select_action(observation_flat, observation_std_flat)\n",
    "\n",
    "    # Reshape the flattened action array back to the original (n_agents, n_dim) shape\n",
    "    actions = action_flat.reshape(env.n_dim, env.n_agents).T  # Reshape to (n_agents, n_dim\n",
    "\n",
    "    return actions  # Return the action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parse_config(config_path)\n",
    "config[\"update_timestep\"] = 25 \n",
    "config[\"decay_rate\"] = 0.99\n",
    "config[\"n_dim\"] = 2\n",
    "config[\"include_gbest\"] = False \n",
    "if config[\"include_gbest\"]:\n",
    "    config[\"obs_dim\"] = 11\n",
    "config[\"n_agents\"] = 10\n",
    "config[\"ep_length\"] = 25\n",
    "config[\"test_ep_length\"] = 100\n",
    "config[\"log_interval\"] = 500\n",
    "config[\"decay_interval\"] = 1000\n",
    "config[\"save_interval\"] = 25000\n",
    "config[\"min_action_std\"] = 0.02\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"update_start\"] = 0\n",
    "config[\"action_std\"] = 0.3\n",
    "config[\"variable_std\"]  = False\n",
    "config[\"reward_scheme\"] = \"FullRewardScheme\"\n",
    "config['use_gbest'] = True\n",
    "config['objective_function'] = \"SphereFunction\"\n",
    "config[\"function_id\"] = \"f01\"\n",
    "config[\"use_optimal_value\"] = True \n",
    "config[\"log_scale\"] = False\n",
    "mode = \"test\"\n",
    "log = False \n",
    "tags = \"new reward training\"\n",
    "env, agent_policy = initialize(config, mode=mode, model_path=model_path_2)\n",
    "neptune_logger = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs = obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_policy.set_action_std(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = get_action(obs, agent_policy, env, random=False)\n",
    "obs, reward, done, info = env.step(actions)\n",
    "obs = obs[0]\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.pbest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPHIVE TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/exp_config.json\"\n",
    "model_path = \"../models/pbest_unfreeze.pth\"\n",
    "model_path_2 = \"../models/gbest.pth\"\n",
    "\n",
    "config = parse_config(config_path)\n",
    "\n",
    "config[\"update_timestep\"] = 1000\n",
    "config[\"decay_rate\"] = 0.95\n",
    "config[\"log_interval\"] = 500\n",
    "config[\"decay_interval\"] = 2000\n",
    "config[\"save_interval\"] = 2000\n",
    "config[\"min_action_std\"] = 0.02\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"update_start\"] = 0\n",
    "config[\"action_std\"] = 0.2\n",
    "config[\"variable_std\"]  = False\n",
    "config[\"reward_scheme\"] = \"FullRewardScheme\"\n",
    "config[\"use_gbest\"] = False\n",
    "\n",
    "deephive_trainer = dh.OptimizationTrainer(config, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deephive_trainer.train_agent(title=\"old_reward_pbest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
