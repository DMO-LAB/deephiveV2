{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephive.optimizers import deephive as dh \n",
    "from deephive.environment.utils import * \n",
    "from datetime import datetime\n",
    "import os \n",
    "import neptune \n",
    "from deephive.environment.deephive_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/exp_config.json\"\n",
    "model_path = \"../models/pbest_unfreeze.pth\"\n",
    "model_path_2 = \"../models/gbest.pth\"\n",
    "\n",
    "def initialize_logger(api_token, tags, config, mode=\"train\"):\n",
    "    run = neptune.init_run(\n",
    "    project=\"DMO-LAB/DeepHive-V2\",\n",
    "    source_files=[\"environment\", \"policies\", \"deephive.py\", \"config\"],\n",
    "    api_token=api_token,\n",
    "    tags=[tags, mode, config[\"objective_function\"], str(config[\"layer_size\"])]\n",
    "    )\n",
    "    neptune_logger = run\n",
    "    return neptune_logger\n",
    "\n",
    "def get_action(observation_info, agent_policy, env, observation_std=None, random=False):\n",
    "    # Ensure observation_info is a numpy array\n",
    "    \n",
    "    if random:\n",
    "        print(\"Random action\")\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    observation = observation_info\n",
    "    if not isinstance(observation, np.ndarray):\n",
    "        observation = np.array(observation)\n",
    "        assert observation.shape[0] == env.n_dim, \"Observation must have the same number of dimensions as the environment\"\n",
    "\n",
    "    # Initialize observation_std with zeros or use provided std, ensuring it matches the shape of observation\n",
    "    if observation_std is None:\n",
    "        observation_std = np.zeros_like(observation)\n",
    "    else:\n",
    "        observation_std = np.array(observation_std)\n",
    "\n",
    "    # Convert observations and stds to the appropriate format (Flatten if necessary)\n",
    "    # Assuming observation and observation_std are 2D arrays of shape (n_agents, n_dim)\n",
    "    # Flatten observation and observation_std for processing\n",
    "    observation_flat = observation.reshape(env.n_agents * env.n_dim, -1)  # Flatten to 1D array\n",
    "    observation_std_flat = observation_std.reshape(-1)  # Flatten to 1D array\n",
    "    # Pass the entire flattened observation and std arrays to select_action\n",
    "    action_flat = agent_policy.select_action(observation_flat, observation_std_flat)\n",
    "\n",
    "    # Reshape the flattened action array back to the original (n_agents, n_dim) shape\n",
    "    actions = action_flat.reshape(env.n_agents, env.n_dim)\n",
    "\n",
    "    return actions  # Return the action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parse_config(config_path)\n",
    "config[\"update_timestep\"] = 25 \n",
    "config[\"decay_rate\"] = 0.95\n",
    "config[\"n_dim\"] = 2\n",
    "config[\"include_gbest\"] = False \n",
    "if config[\"include_gbest\"]:\n",
    "    config[\"obs_dim\"] = 11\n",
    "config[\"n_agents\"] = 10\n",
    "config[\"ep_length\"] = 25\n",
    "config[\"log_interval\"] = 500\n",
    "config[\"decay_interval\"] = 1000\n",
    "config[\"save_interval\"] = 25000\n",
    "config[\"min_action_std\"] = 0.02\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"update_start\"] = 0\n",
    "config[\"action_std\"] = 0.3\n",
    "config[\"variable_std\"]  = False\n",
    "config[\"reward_scheme\"] = \"FullRewardScheme\"\n",
    "config['use_gbest'] = False\n",
    "config['objective_function'] = \"CosineMixtureFunction\"\n",
    "config[\"function_id\"] = 2\n",
    "config[\"use_optimal_value\"] = True \n",
    "config[\"log_scale\"] = False\n",
    "mode = \"train\"\n",
    "log = False \n",
    "tags = \"new reward training\"\n",
    "env, agent_policy = initialize(config, mode=mode, model_path=model_path)\n",
    "neptune_logger = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "opt_params=np.array([[-55.2763985 , -70.42955972]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.scaler_helper.scale(a_state[:,-1], env.worst_obj_value, -10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_policy.action_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_agent(env, agent_policy, title, config, neptune_logger, n_episodes=None, update_timestep=None, decay_rate=None, log_interval=None, decay_interval=None, save_interval=None, min_action_std=None):\n",
    "    # if parameters are not provided, use the ones from the config file\n",
    "    if update_timestep is None:\n",
    "        update_timestep = config[\"update_timestep\"]\n",
    "    if decay_rate is None:\n",
    "        decay_rate =  config[\"decay_rate\"]\n",
    "    if log_interval is None:\n",
    "        log_interval =  config[\"log_interval\"]\n",
    "    if decay_interval is None:\n",
    "        decay_interval =  config[\"decay_interval\"]\n",
    "    if save_interval is None:\n",
    "        save_interval =  config[\"save_interval\"]\n",
    "    if min_action_std is None:\n",
    "        min_action_std =  config[\"min_action_std\"]\n",
    "    if n_episodes is None:\n",
    "        n_episodes =  config[\"n_episodes\"]\n",
    "\n",
    "    average_returns = []\n",
    "    training_run_title = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    training_run_title = title + \"_\" + training_run_title\n",
    "    save_path = f\"training_runs/{training_run_title}/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    timesteps = 0\n",
    "    for episode in range(0, n_episodes+1):\n",
    "        # if timesteps < config[\"update_start\"]:\n",
    "        #     print(f\"Using random actions for episode {episode} - timesteps: {timesteps}\")\n",
    "        # if episode % log_interval == 0:\n",
    "        #     print(f\"Episode: {episode}, timesteps: {timesteps}\")\n",
    "        observation_info =  env.reset()\n",
    "        obs, obs_std = observation_info \n",
    "        episode_return = np.zeros( env.n_agents)\n",
    "        for step in range( env.ep_length):\n",
    "            # if timesteps < config[\"update_start\"]:\n",
    "            #     actions =  get_action(obs, agent_policy, env, obs_std, random=True)\n",
    "            # else:\n",
    "                # print(f\"Using policy actions for episode {episode} - timesteps: {timesteps}\")\n",
    "            actions =  get_action(obs, agent_policy, env, obs_std, random=False)\n",
    "            observation_info, reward, done, info =  env.step(actions)\n",
    "            obs, obs_std = observation_info\n",
    "            # add reward to agent buffer\n",
    "            for ag in range( env.n_agents):\n",
    "                 agent_policy.buffer.rewards += [reward[ag]] *  env.n_dim\n",
    "                 agent_policy.buffer.is_terminals += [done[ag]] *  env.n_dim\n",
    "            \n",
    "            episode_return += reward\n",
    "            if  neptune_logger and episode % log_interval == 0:\n",
    "                # log global best agent value\n",
    "                neptune_logger[f\"train/global_best_value/episode{episode}\"].log(float(info[\"gbest\"][-1]))\n",
    "                    \n",
    "            if step ==  env.ep_length - 1:\n",
    "                average_returns.append(np.mean(episode_return))\n",
    "                running_average_rewards = np.mean(average_returns)\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[\"train/average_return\"].log(average_returns[-1])\n",
    "                    neptune_logger[\"train/percentage_high_std\"].log( env.surrogate.percent_high_std)\n",
    "            timesteps += 1\n",
    "        \n",
    "        if timesteps % update_timestep == 0:\n",
    "            #print(f\"Updating policy at timestep {timesteps}\")\n",
    "            agent_policy.update()\n",
    "    \n",
    "        if timesteps > 0 and episode % log_interval == 0:\n",
    "            print_items(\n",
    "                    episode = episode,\n",
    "                    average_returns = average_returns[-1],\n",
    "                    timesteps = timesteps,\n",
    "                    )\n",
    "            # if env.use_surrogate:\n",
    "            #     env.surrogate.plot_checkpoints_state(f\"{save_path}surrogate-checkpoint-{episode}.png\")\n",
    "            #     env.surrogate.plot_variance(f\"{save_path}variance-{episode}.png\")\n",
    "            #     env.surrogate.plot_surrogate(f\"{save_path}surrogate-{episode}.png\")\n",
    "            #     if  neptune_logger:\n",
    "            #         neptune_logger[f\"train/plots/surrogate/surrogate-{episode}\"].upload(f\"{save_path}surrogate-{episode}.png\")\n",
    "            #         neptune_logger[f\"train/plots/variance/variance-{episode}\"].upload(f\"{save_path}variance-{episode}.png\")\n",
    "            #         neptune_logger[f\"train/plots/surrogate_checker/surrogate-checkpoint-{episode}\"].upload(f\"{save_path}surrogate-checkpoint-{episode}.png\")\n",
    "                \n",
    "\n",
    "            if env.n_dim <= 2 and episode % log_interval == 0:\n",
    "                env.render(file_path=f\"{save_path}{episode}.gif\", type=\"history\")\n",
    "                if neptune_logger:\n",
    "                    neptune_logger[f\"train/gifs/{episode}.gif\"].upload(f\"{save_path}{episode}.gif\")\n",
    "                    \n",
    "        if timesteps % decay_interval == 0:\n",
    "            print(f\"Decaying action std at timestep {timesteps}\")\n",
    "            agent_policy.decay_action_std(decay_rate, min_action_std=min_action_std, debug=True)\n",
    "            \n",
    "        if timesteps % save_interval == 0 and timesteps > 0:\n",
    "            if average_returns[-1] > running_average_rewards:\n",
    "                print(f\"Average return: {average_returns[-1]}, running average: {running_average_rewards}\")\n",
    "                agent_policy.save(save_path, episode=timesteps)\n",
    "                if  neptune_logger:\n",
    "                    neptune_logger[f\"train/checkpoints/timesteps-{timesteps}\"].upload(f\"{save_path}/policy-{timesteps}.pth\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(env, agent_policy, \"cosine_mixture\", config, neptune_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.447 , 0.4622, 0.8691],\n",
       "       [0.7086, 0.2081, 0.5039],\n",
       "       [0.482 , 0.5702, 0.8642],\n",
       "       [0.839 , 0.7584, 0.3159],\n",
       "       [0.475 , 0.52  , 0.9636],\n",
       "       [0.01  , 0.481 , 0.1858],\n",
       "       [0.2742, 0.845 , 0.4267],\n",
       "       [0.8721, 0.6645, 0.4772],\n",
       "       [0.4423, 0.1608, 0.4935],\n",
       "       [0.7877, 0.2076, 0.263 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.opt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11, -0.08,  0.01],\n",
       "       [ 0.42, -0.58, -0.52],\n",
       "       [-0.04,  0.14,  0.  ],\n",
       "       [ 0.68,  0.52, -0.79],\n",
       "       [-0.05,  0.04,  0.15],\n",
       "       [-0.98, -0.04, -0.97],\n",
       "       [-0.45,  0.69, -0.63],\n",
       "       [ 0.74,  0.33, -0.55],\n",
       "       [-0.12, -0.68, -0.53],\n",
       "       [ 0.58, -0.58, -0.86]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(env._get_actual_state(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.observation_schemes.generate_observation(pbest=env.pbest.copy(), use_gbest=env.use_gbest, ratio=env.split_ratio, include_gbest=env.include_gbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = observation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = get_action(obs, agent_policy, env, random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state_history[1, :, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent_policy.buffer.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent_policy.buffer.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPHIVE TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/exp_config.json\"\n",
    "model_path = \"../models/pbest_unfreeze.pth\"\n",
    "model_path_2 = \"../models/gbest.pth\"\n",
    "\n",
    "config = parse_config(config_path)\n",
    "\n",
    "config[\"update_timestep\"] = 1000\n",
    "config[\"decay_rate\"] = 0.95\n",
    "config[\"log_interval\"] = 500\n",
    "config[\"decay_interval\"] = 2000\n",
    "config[\"save_interval\"] = 2000\n",
    "config[\"min_action_std\"] = 0.02\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"update_start\"] = 0\n",
    "config[\"action_std\"] = 0.2\n",
    "config[\"variable_std\"]  = False\n",
    "config[\"reward_scheme\"] = \"FullRewardScheme\"\n",
    "config[\"use_gbest\"] = False\n",
    "\n",
    "deephive_trainer = dh.OptimizationTrainer(config, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deephive_trainer.train_agent(title=\"old_reward_pbest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
