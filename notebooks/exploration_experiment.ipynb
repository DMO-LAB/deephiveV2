{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from policies.mappo import MAPPO\n",
    "from environment.optimization_environment import OptimizationEnv\n",
    "from environment.utils import parse_config\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import neptune\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from other_algorithms.pso import ParticleSwarmOptimizer\n",
    "import time\n",
    "from scipy.stats import multivariate_normal\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(config_path, mode=\"train\", **kwargs):\n",
    "    env = OptimizationEnv(config_path)\n",
    "    agent_policy = MAPPO(config_path)\n",
    "    if mode == \"test\" or mode == \"benchmark\":\n",
    "        model_path = kwargs.get(\"model_path\", None)\n",
    "        if model_path is None:\n",
    "            raise ValueError(\"Model path must be provided for testing\")\n",
    "        agent_policy.load(model_path)\n",
    "    return env, agent_policy\n",
    "\n",
    "def print_items(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key, value)\n",
    "        \n",
    "def get_action(observation_info, agent_policy, env):\n",
    "    observation, observation_std = observation_info\n",
    "    actions = np.zeros((env.n_agents, env.n_dim))\n",
    "    for dim in range(env.n_dim):\n",
    "        observation[dim] = observation[dim].astype(np.float32)\n",
    "        #print(observation[dim])\n",
    "        observation_std[dim] = observation_std[dim].astype(np.float32)\n",
    "        action = agent_policy.select_action(observation[dim], observation_std[dim])\n",
    "        actions[:, dim] = action\n",
    "    return actions\n",
    "\n",
    "# def cal_densities(points):\n",
    "#     densities = np.array([multivariate_normal(mean=mean, cov=cov, allow_singular=True).pdf(points)\n",
    "#                               for mean, cov in zip(env.gmm.gmm.means_, env.gmm.gmm.covariances_)])\n",
    "#     return densities\n",
    "\n",
    "# def cal_densities_variances(densities):\n",
    "#     variances = []\n",
    "#     for dim in range(densities.shape[0]):\n",
    "#         densities_variances = np.var(densities[dim], axis=0)\n",
    "#         print(densities_variances)\n",
    "#         variances.append(densities_variances)\n",
    "#     return variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config/config.json'\n",
    "model_path = \"models/model.pth\"\n",
    "mode = \"train\"\n",
    "env, agent_policy = initialize(config_path, mode=mode, model_path=model_path)\n",
    "config = parse_config(config_path)\n",
    "agent_policy.set_action_std(config[\"test_action_std\"])\n",
    "\n",
    "def display_gmm_details(gmm):\n",
    "    print(\"Means: \", gmm.means_)\n",
    "    print(\"Covariances: \", gmm.covariances_)\n",
    "    print(\"Weights: \", gmm.weights_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_info = env.reset()\n",
    "rewards = []\n",
    "std_comp = []\n",
    "hig_std_comp = []\n",
    "iters = 11\n",
    "results_buffer = np.zeros((iters, 4))\n",
    "for i in range(iters):\n",
    "    print(\"Iteration: \", i)\n",
    "    # if i % 1 == 0:\n",
    "        # _, std = env.surrogate.evaluate(env.surrogate.checkpoints)\n",
    "        # _ = env.surrogate.cal_reward(\n",
    "        # std_comp.append(1-std_std)\n",
    "        # print(\"Reward: \", reward)\n",
    "        # print(\"Std: \", std_std)\n",
    "        # print(\"total_reward: \", reward + (1 - std_std))\n",
    "        # combined_reward = reward * np.exp(-0.1 * max(0, 1- std_std))\n",
    "        # results_buffer[i] = [reward, (1 - std_std), reward * (1 - std_std), combined_reward]\n",
    "        # # env.surrogate.plot_variance()\n",
    "        \n",
    "        # rewards.append(reward + (1 - std_std))\n",
    "        \n",
    "    #action =  get_action(observation_info, agent_policy, env)\n",
    "    action = np.random.uniform(-0.3, 0.3, (env.n_agents, env.n_dim))\n",
    "    observation_info, reward, done, info = env.step(action)\n",
    "    rewards.append(np.mean(reward)) \n",
    "    env.surrogate.plot_checkpoints_state()\n",
    "    print(\"Reward: \", np.mean(reward))\n",
    "    print(1 - env.surrogate.percent_high_std / 100)\n",
    "    std_std = np.std(env.surrogate.checkpoints_std)\n",
    "    print(f\"Std - {std_std}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.surrogate.plot_checkpoints_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(type=\"history\", file_path=\"new_history.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.plot(results_buffer[:, 2], label=\"total reward\")\n",
    "plt.plot(results_buffer[:, 0], label=\"reward\")\n",
    "plt.plot(results_buffer[:, 1], label=\"std\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = env.surrogate.evaluate(env.surrogate.checkpoints)\n",
    "mean2, std2 = env.surrogate.evaluate(env.surrogate.checkpoints)\n",
    "\n",
    "# check if the mean and std are the same\n",
    "print(np.all(mean == mean2))\n",
    "print(np.all(std == std2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
