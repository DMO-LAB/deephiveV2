{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deephive.environment.utils import parse_config\n",
    "from datetime import datetime\n",
    "import os \n",
    "import numpy as np\n",
    "# from deephive.environment.deephive_utils import *\n",
    "import torch\n",
    "from deephive.environment.optimization_environment import OptimizationEnv\n",
    "from deephive.policies.mappo import MAPPO\n",
    "from deephive.train_test_utils import train, test, initialize, initialize_logger, get_action, get_direct_action, mean_confidence_interval\n",
    "# Set print options\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "from dotenv import load_dotenv\n",
    "api_token = os.environ.get(\"NEPTUNE_API_TOKEN\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/exp_config.json\"\n",
    "model_path = \"../models/pbest_unfreeze.pth\"\n",
    "model_path_2 = \"../models/gbest.pth\"\n",
    "config = parse_config(config_path)\n",
    "config['use_gbest'] = True\n",
    "config[\"use_optimal_value\"] = True\n",
    "config[\"log_scale\"] = False \n",
    "config[\"include_gbest\"] = False\n",
    "config[\"negative\"] = True\n",
    "if config[\"include_gbest\"]:\n",
    "    config[\"obs_dim\"] = 11\n",
    "else:\n",
    "    config[\"obs_dim\"] = 9\n",
    "config[\"ep_length\"] = 25\n",
    "\n",
    "config[\"min_action_std\"] = 0.001\n",
    "config[\"action_std\"] = 0.2\n",
    "config[\"variable_std\"]  = False\n",
    "config[\"update_timestep\"] = 2\n",
    "config[\"decay_rate\"] = 0.99\n",
    "config[\"log_interval\"] = 500\n",
    "config[\"decay_interval\"] = 5\n",
    "config[\"save_interval\"] = 200\n",
    "mode = \"train\"\n",
    "config[\"test_decay_rate\"] = 0.9\n",
    "config[\"test_decay_start\"] = 0\n",
    "\n",
    "config[\"reward_scheme\"] = \"FullRewardScheme2\"\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"plot_gif\"] = True\n",
    "config[\"plot_gbest\"] = True\n",
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/Users/elotech/Desktop/DeepHiveV2/notebooks/training_results/experiment_4/policy-0.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dimensions = [2]\n",
    "dim_agents = {\n",
    "    2: 50,\n",
    "    5: 50,\n",
    "    10: 100,\n",
    "    20: 100,\n",
    "    30: 40,\n",
    "    50: 100\n",
    "}\n",
    "\n",
    "#function_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "function_ids = [\"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\"]\n",
    "# function_opt_val = 100 * np.array(function_ids) + 100\n",
    "config[\"test_ep_length\"] = 100\n",
    "config[\"n_agents\"] = 10\n",
    "config[\"n_dim\"] = 2\n",
    "config['objective_function'] = \"CosineMixtureFunction\" \n",
    "config[\"function_id\"] = \"f01\"\n",
    "env, agent_policy = initialize(config, mode=mode, model_path=[model_path, model_path_2])\n",
    "if mode == \"test\":\n",
    "    pbest_policy, gbest_policy = agent_policy[0], agent_policy[1]\n",
    "else:\n",
    "    agent_policy = agent_policy\n",
    "obs, obs_std = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for dim in dimensions:\n",
    "#     config[\"n_agents\"] = dim_agents[dim]\n",
    "#     config[\"n_dim\"] = dim\n",
    "#     for function_id in function_ids[0:3]:\n",
    "#         config[\"function_id\"] = function_id\n",
    "#         env, agent_policy = initialize(config, mode=mode, model_path=MODEL_PATH)\n",
    "#         # if mode == \"test\":\n",
    "#         #     pbest_policy, gbest_policy = agent_policy[0], agent_policy[1]\n",
    "#         #     agent_policy = pbest_policy\n",
    "#         # else:\n",
    "#         #     agent_policy = agent_policy\n",
    "#         agent_policy.load(MODEL_PATH)\n",
    "#         try:\n",
    "#             save_path = f\"test_results/{date}/results{dim}D/function_{function_id}/\"\n",
    "#             os.makedirs(save_path, exist_ok=True)\n",
    "#             optimal_positions = None\n",
    "#             gbest_values = test(env, agent_policy, config, n_episodes=10, ep_length=config[\"test_ep_length\"],\n",
    "#                                 decay_rate=0.97, decay_start=20, debug=False, show_interval=10, \n",
    "#                                 fps=5, optimal_positions=optimal_positions, save_path=save_path, \n",
    "#                                 randomize=False)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, agent_policy = initialize(config, mode=\"train\", model_path=\"../models/gbest.pth\")\n",
    "agent_policy.load(MODEL_PATH)\n",
    "agent_policy.set_action_std(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(observation, agent_policy, env, observation_std=None, **kwargs):\n",
    "    # Ensure observation_info is a numpy array\n",
    "    \n",
    "    if not isinstance(observation, np.ndarray):\n",
    "        observation = np.array(observation)\n",
    "        assert observation.shape[0] == env.n_dim, \"Observation must have the same number of dimensions as the environment\"\n",
    "\n",
    "    # Initialize observation_std with zeros or use provided std, ensuring it matches the shape of observation\n",
    "    if observation_std is None:\n",
    "        observation_std = np.zeros_like(observation)\n",
    "    else:\n",
    "        observation_std = np.array(observation_std)\n",
    "\n",
    "    # Flatten the observation and std arrays\n",
    "    observation_flat = observation.reshape(env.n_dim * env.n_agents, -1)  # Flatten to 1D array\n",
    "    observation_std_flat = observation_std.reshape(-1)  # Flatten to 1D array\n",
    "    # Pass the entire flattened observation and std arrays to select_action\n",
    "    action_flat = agent_policy.select_action(observation_flat, observation_std_flat)\n",
    "\n",
    "    # Reshape the flattened action array back to the original (n_agents, n_dim) shape\n",
    "    actions = action_flat.reshape(env.n_dim, env.n_agents).T  # Reshape to (n_agents, n_dim\n",
    "\n",
    "    return actions  # Return the action\n",
    "\n",
    "\n",
    "def get_direct_action(obs, obs_std, agent_policy):\n",
    "    torch_obs = torch.FloatTensor(obs)\n",
    "    torch_obs_std = torch.FloatTensor(obs_std)\n",
    "    action = agent_policy.policy.act(torch_obs, torch_obs_std)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = agent_policy\n",
    "ep_length = 10\n",
    "decay_start = 20\n",
    "restart_interval = 5\n",
    "current_action_std = 0.0001\n",
    "policy.set_action_std(current_action_std)   \n",
    "obs = env.reset()\n",
    "obs, obs_std = obs\n",
    "gbest_values = []\n",
    "# for step in range(ep_length):\n",
    "    \n",
    "    # if step <= decay_start or step % restart_interval == 0:\n",
    "    #     actions = get_action(obs, policy, env, obs_std, random=True) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_actual_state()[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = get_action(obs, policy, env, obs_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _, _, _ = env.step(actions)\n",
    "obs = obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = get_action(obs, policy, env, obs_std)\n",
    "obs, _, _, _ = env.step(actions)\n",
    "obs, obs_std = obs\n",
    "gbest_values.append(env.gbest[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_actual_state()[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_actual_state()[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.objective_function.tracker.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only f5:\n",
    "from cec2017.functions import f5\n",
    "import numpy as np\n",
    "samples = 3\n",
    "dimension = 50\n",
    "x = np.random.uniform(-100, 100, size=(samples, dimension))\n",
    "val = f5(x)\n",
    "for i in range(samples):\n",
    "    print(f\"f5(x_{i}) = {val[i]:.6f}\")\n",
    "\n",
    "# Using all functions:\n",
    "from cec2017.functions import all_functions\n",
    "for f in all_functions:\n",
    "    x = np.random.uniform(-100, 100, size=(samples, dimension))\n",
    "    val = f(x)\n",
    "    for i in range(samples):\n",
    "        print(f\"{f.__name__}(x_{i}) = {val[i]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_actual_state()[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gbest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_path = \"test_results/2024-02-29_15-29-49/\"\n",
    "ndim = 10\n",
    "test_data_path = base_data_path + f\"results{ndim}D\"\n",
    "df = pd.DataFrame(columns=[\"function_id\", \"mean\", \"lower\", \"upper\", \"optimum\"])\n",
    "for function_id in function_ids:\n",
    "    try:\n",
    "        data_path = f\"{test_data_path}/function_{function_id}/gbest_values.npy\"\n",
    "        gbest_values = np.load(data_path) * -1\n",
    "        mean_val, lower_val, upper_val = mean_confidence_interval(gbest_values)\n",
    "        function_result = [f\"function_{function_id}\", mean_val[-1], lower_val[-1], upper_val[-1], function_opt_val[function_id]]\n",
    "        df.loc[len(df)] = function_result\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        continue\n",
    "\n",
    "df.to_csv(f\"{base_data_path}/results{ndim}D.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_p = -100\n",
    "max_p = 100\n",
    "x = np.linspace(min_p, max_p, 100)\n",
    "y = np.linspace(min_p, max_p, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i,j] = np.log10(-env.objective_function.evaluate(np.array([[X[i,j], Y[i,j]]])))\n",
    "        print(Z[i,j])\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a 2D surface plot showing the value of the objective function at different points in the search space\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.contourf(X, Y, Z, cmap='viridis')\n",
    "# add a grid    \n",
    "ax.grid(True)\n",
    "# take 20 evenly spaced samples from the search space and write the value of the objective function at each point\n",
    "# x = np.linspace(min_p, max_p, 100)\n",
    "# y = np.linspace(min_p, max_p, 100)\n",
    "# X, Y = np.meshgrid(x, y)\n",
    "# Z = np.zeros_like(X)\n",
    "# for i in range(X.shape[0]):\n",
    "#     for j in range(X.shape[1]):\n",
    "#         Z[i,j] = np.log10(-env.objective_function.evaluate(np.array([[X[i,j], Y[i,j]]])))\n",
    "# ax.scatter(X, Y, c='r')\n",
    "# for i in range(X.shape[0]):\n",
    "#     for j in range(X.shape[1]):\n",
    "#         ax.text(X[i,j], Y[i,j], f'{Z[i,j]:.2f}')\n",
    "# show color bar\n",
    "plt.colorbar(ax.contourf(X, Y, Z, cmap='viridis'))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "\n",
    "config['use_gbest'] = False\n",
    "config[\"use_optimal_value\"] = True\n",
    "config[\"log_scale\"] = True\n",
    "config[\"include_gbest\"] = True\n",
    "if config[\"include_gbest\"]:\n",
    "    config[\"obs_dim\"] = 5\n",
    "else:\n",
    "    config[\"obs_dim\"] = 4\n",
    "config[\"ep_length\"] = 25\n",
    "\n",
    "config[\"min_action_std\"] = 0.00001\n",
    "config[\"action_std\"] = 0.3\n",
    "config[\"variable_std\"]  = False\n",
    "config[\"update_timestep\"] = 2\n",
    "config[\"decay_rate\"] = 0.99\n",
    "config[\"log_interval\"] = 500\n",
    "config[\"decay_interval\"] = 5\n",
    "config[\"save_interval\"] = 200\n",
    "mode = \"train\"\n",
    "config[\"test_decay_rate\"] = 0.9\n",
    "config[\"test_decay_start\"] = 0\n",
    "\n",
    "config[\"reward_scheme\"] = \"FullRewardScheme2\"\n",
    "config[\"n_episodes\"] = 5000\n",
    "config[\"plot_gif\"] = True\n",
    "config[\"plot_gbest\"] = True\n",
    "\n",
    "\n",
    "config[\"n_agents\"] = 10\n",
    "config[\"n_dim\"] = 2\n",
    "config['objective_function'] = \"SphereFunction\" \n",
    "config[\"function_id\"] = 6\n",
    "env, agent_policy = initialize(config, mode=mode, model_path=[model_path, model_path_2])\n",
    "if mode == \"test\":\n",
    "    pbest_policy, gbest_policy = agent_policy[0], agent_policy[1]\n",
    "else:\n",
    "    agent_policy = agent_policy\n",
    "obs, obs_std = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_logger = initialize_logger(api_token, \"Deephive\", config, mode=mode)\n",
    "agent_policy = train(env, agent_policy, config, title=\"experiment_9\", neptune_logger=neptune_logger)\n",
    "print(agent_policy.action_std)\n",
    "if neptune_logger is not None:\n",
    "    neptune_logger.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_policy.load(\"training_results/experiment_9/policy-4800.pth\")\n",
    "# agent_policy.set_action_std(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, obs_std = env.reset()\n",
    "# actions = get_action(obs, agent_policy, env, obs_std, random=False, pso_action=False, w=0.5, c1=1.5, c2=1.0)\n",
    "# # print(actions)\n",
    "\n",
    "# env.render()\n",
    "# obs, _, _, _ = env.step(actions)\n",
    "# obs, obs_std = obs\n",
    "# env.render()\n",
    "# actions = get_action(obs, agent_policy, env, obs_std, random=False, pso_action=False, w=0.5, c1=1.5, c2=1.0)\n",
    "# print(actions)\n",
    "# print(env._get_actual_state()[:, -1])\n",
    "# print(env.state[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_path = f\"test_results/{date}/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "optimal_positions = None\n",
    "env.n_agents = 100\n",
    "gbest_values = test(env, agent_policy, config, n_episodes=20, ep_length=100,\n",
    "                    decay_rate=0.95, decay_start=20, debug=False, show_interval=5, \n",
    "                    fps=5, optimal_positions=optimal_positions, save_path=save_path, \n",
    "                    randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_actual_state()[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.n_dim, env.n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.n_agents)\n",
    "env.n_agents = 100\n",
    "_ = env.reset()\n",
    "print(env.n_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg, upper, lower = mean_confidence_interval(np.array(gbest_values))\n",
    "print(avg[-1], upper[-1], lower[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(len(gbest_values))\n",
    "print(f\"Index: {index}\")\n",
    "gbest_values[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.objective_function.tracker.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSO VARIANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"test_ep_length\"] = 50\n",
    "config[\"ep_length\"] = 100\n",
    "config[\"n_agents\"] = 100\n",
    "config[\"n_dim\"] = 2\n",
    "config['objective_function'] = \"CEC17\" \n",
    "config[\"function_id\"] = 0\n",
    "config[\"log_scale\"] = True\n",
    "config[\"action_std\"] = 0.02\n",
    "env, agent_policy = initialize(config, mode=mode, model_path=[model_path, model_path_2])\n",
    "if mode == \"test\":\n",
    "    pbest_policy, gbest_policy = agent_policy[0], agent_policy[1]\n",
    "else:\n",
    "    agent_policy = agent_policy\n",
    "obs, obs_std = env.reset()\n",
    "MODEL_PATH = \"training_results/experiment_9/policy-4800.pth\"\n",
    "agent_policy.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(observation, agent_policy, env, observation_std=None, random=False, env_random=False, pso_action=False, **kwargs):\n",
    "    # Ensure observation_info is a numpy array\n",
    "    \n",
    "    if env_random:\n",
    "        current_state =  env.state[:, :2]\n",
    "        env._generate_init_state(count=True)\n",
    "        new_state = env.state[:, :2]\n",
    "        action = new_state - current_state\n",
    "        return action\n",
    "    \n",
    "    if random:\n",
    "        # Generate random actions\n",
    "        action = np.random.uniform(-1, 1, size=(env.n_agents, env.n_dim))\n",
    "        return action\n",
    "        \n",
    "    \n",
    "    if not isinstance(observation, np.ndarray):\n",
    "        observation = np.array(observation)\n",
    "        assert observation.shape[0] == env.n_dim, \"Observation must have the same number of dimensions as the environment\"\n",
    "\n",
    "    if pso_action:\n",
    "        # grab the pso parameters from kwargs\n",
    "        w = kwargs.get(\"w\", 0.5)\n",
    "        c1 = kwargs.get(\"c1\", 1.5)\n",
    "        c2 = kwargs.get(\"c2\", 1.0)\n",
    "        print(f\"PSO action with w={w}, c1={c1}, c2={c2}\")\n",
    "        # grab the second, fourth and eight element of the observation for each dimension\n",
    "        # that represents the previous velocity, best position and global best position\n",
    "        # for each particle\n",
    "        p_vel = observation[:, :,1]\n",
    "        p_best = observation[:,:,3]\n",
    "        g_best = observation[:,:,9]\n",
    "        \n",
    "        rp = np.random.uniform(-1, 1,size=env.n_dim)\n",
    "        rg = np.random.uniform(-1, 1,size=env.n_dim)\n",
    "        \n",
    "        v = w * p_vel.T + c1 * rp * p_best.T + c2 * rg * g_best.T\n",
    "        return v\n",
    "    # Initialize observation_std with zeros or use provided std, ensuring it matches the shape of observation\n",
    "    if observation_std is None:\n",
    "        observation_std = np.zeros_like(observation)\n",
    "    else:\n",
    "        observation_std = np.array(observation_std)\n",
    "\n",
    "    # Flatten the observation and std arrays\n",
    "    observation_flat = observation.reshape(env.n_dim * env.n_agents, -1)  # Flatten to 1D array\n",
    "    observation_std_flat = observation_std.reshape(-1)  # Flatten to 1D array\n",
    "    # Pass the entire flattened observation and std arrays to select_action\n",
    "    action_flat = agent_policy.select_action(observation_flat, observation_std_flat)\n",
    "\n",
    "    # Reshape the flattened action array back to the original (n_agents, n_dim) shape\n",
    "    actions = action_flat.reshape(env.n_dim, env.n_agents).T  # Reshape to (n_agents, n_dim\n",
    "\n",
    "    return actions  # Return the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_policy.set_action_std(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iters = 100\n",
    "obs, obs_std = env.reset()\n",
    "action_prob = 0.7\n",
    "max_std = 0.2\n",
    "min_std = 0.000001\n",
    "\n",
    "for i in range(iters):\n",
    "    actions = get_action(obs, agent_policy, env, obs_std)\n",
    "    #pso_actions = get_action(obs, agent_policy, env, obs_std, random=True, pso_action=True, w=0.3, c1=0.5, c2=0.6)\n",
    "    \n",
    "    #This commented out code block is implementing a way to adjust the standard deviation of the actions taken by the agent based on a probability threshold.\n",
    "    if np.random.random() > action_prob:\n",
    "        agent_policy.set_action_std(max_std)\n",
    "    else:\n",
    "        agent_policy.set_action_std(min_std)\n",
    "    \n",
    "    obs, _, _, _ = env.step(actions)\n",
    "    obs, obs_std = obs\n",
    "\n",
    "print(env.gbest[-1])\n",
    "    \n",
    "# env.render()\n",
    "env._get_actual_state()[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(type=\"history\", file_path=\"save_gif3.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
