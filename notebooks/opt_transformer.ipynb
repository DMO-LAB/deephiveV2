{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from deephive.environment.optimization_functions.benchmark_functions import *\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_selector = FunctionSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Render:\n",
    "    \"\"\" Helper class for rendering the environment. \n",
    "        The class should be able to plot the particles actual position in the optimization function landscape if it is \n",
    "        a 1D or 2D function. Also the class should be able to plot the state history as a gif or video.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "        \n",
    "    def render_state(self, file_path: Optional[str] = None, **kwargs):\n",
    "        self.optimal_position = kwargs.get(\"optimal_positions\", None)\n",
    "        if self.env.dimension > 2:\n",
    "            raise ValueError(\"Cannot render state for n_dim > 2\")\n",
    "        \n",
    "        self._render_state_2d(file_path)\n",
    "            \n",
    "    def _render_state_2d(self, file_path: Optional[str] = None):\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.linspace(self.env.bounds[0], self.env.bounds[1], 1000)\n",
    "        y = np.linspace(self.env.bounds[0], self.env.bounds[1], 1000)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        if self.env.log_scale: \n",
    "            Z = np.log10(-self.env.fitness_function(np.array([X.flatten(), Y.flatten()]).T).reshape(X.shape))\n",
    "        else:\n",
    "            Z = self.env.fitness_function(np.array([X.flatten(), Y.flatten()]).T).reshape(X.shape)\n",
    "        ax.contour(X, Y, Z, 50)\n",
    "        ax.set_xlim(self.env.bounds[0], self.env.bounds[1])\n",
    "        ax.set_ylim(self.env.bounds[0], self.env.bounds[1])\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_title(\"Particle positions\")\n",
    "        #state = self.env._get_actual_state()\n",
    "        state = self.env.state_history[:, self.env.current_step, :-1]\n",
    "        ax.scatter(state[:, 0], state[:, 1], c=\"red\", s=100, marker=\"^\", edgecolors=\"black\", label=\"Particle's points\", alpha=1)\n",
    "        # plot the optimal position if given\n",
    "        if self.optimal_position is not None:\n",
    "            ax.scatter(self.optimal_position[0], self.optimal_position[1], c=\"blue\", s=300, marker=\"o\", edgecolors=\"black\", label=\"Optimal Position\")\n",
    "        \n",
    "        ax.legend()\n",
    "        \n",
    "        \n",
    "        if file_path is not None:\n",
    "            plt.savefig(file_path)\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "    def render_state_history(self, file_path: str, fps: int = 10, **kwargs):\n",
    "        self.optimal_position = kwargs.get(\"optimal_positions\", None)\n",
    "        if self.env.dimension > 2:\n",
    "            raise ValueError(\"Cannot render state for n_dim > 2\")\n",
    "        \n",
    "        self._render_state_history_2d(file_path, fps)\n",
    "            \n",
    "        \n",
    "    def _render_state_history_2d(self, file_path: str, fps: int):\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.linspace(self.env.bounds[0], self.env.bounds[1], 1000)\n",
    "        y = np.linspace(self.env.bounds[0], self.env.bounds[1], 1000)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        if self.env.log_scale:\n",
    "            Z = np.log10(-self.env.fitness_function(np.array([X.flatten(), Y.flatten()]).T).reshape(X.shape))\n",
    "        else:\n",
    "            Z = self.env.fitness_function(np.array([X.flatten(), Y.flatten()]).T).reshape(X.shape)\n",
    "        ax.contour(X, Y, Z, 50)\n",
    "        if self.optimal_position is not None:\n",
    "            ax.scatter(self.optimal_position[0], self.optimal_position[1], c=\"green\", s=300, marker=\"o\", edgecolors=\"black\", label=\"Optimal Position\")\n",
    "        ax.set_xlim(self.env.bounds[0], self.env.bounds[1])\n",
    "        ax.set_ylim(self.env.bounds[0], self.env.bounds[1])\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_title(\"Particle positions\")\n",
    "        ax.legend()\n",
    "        scat = ax.scatter([], [], c=\"red\", s=100, marker=\"^\", edgecolors=\"black\")\n",
    "        # add a text box to display the iteration number\n",
    "        text = ax.text(0.05, 0.95, \"\", transform=ax.transAxes)\n",
    "        self.previous_state_history = self.env.state_history[:, 0, :-1]\n",
    "        \n",
    "        def animate(i):\n",
    "            scat.set_offsets(self.env.state_history[:, i, :-1])\n",
    "            text.set_text(f\"Iteration: {i}\")\n",
    "            # plot a line between the previous state and the current state\n",
    "            for j in range(len(self.env.state_history)):\n",
    "                ax.plot([self.previous_state_history[j, 0], self.env.state_history[j, i, 0]], [self.previous_state_history[j, 1], self.env.state_history[j, i, 1]], c=\"black\", alpha=0.1)\n",
    "                \n",
    "            self.previous_state_history = self.env.state_history[:, i, :-1]\n",
    "            # clear the line between the previous state and the current state\n",
    "        \n",
    "            # use different colors for the particles based on their role - red for closer half, blue for farther half\n",
    "    \n",
    "            return scat,\n",
    "        print(\"Creating animation\")\n",
    "        anim = animation.FuncAnimation(fig, animate, frames=self.env.state_history.shape[1], interval=1000/fps, blit=True)\n",
    "        print(\"Saving animation to: \", file_path)\n",
    "        anim.save(file_path, writer=\"Pillow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScalerHelper:\n",
    "    \"\"\" Helper class to scale and unscale observations and actions \"\"\"\n",
    "    @staticmethod\n",
    "    def scale_observation(observation: np.ndarray, low: float, high: float) -> np.ndarray:\n",
    "        # Scale the observation to the range [0, 1]\n",
    "        scaled_obs = (observation - low) / (high - low)\n",
    "        # Stretch and shift the [0, 1] interval to [-1, 1]\n",
    "        return scaled_obs\n",
    "    \n",
    "    @staticmethod\n",
    "    def unscale_observation(observation: np.ndarray, low: float, high: float) -> np.ndarray:\n",
    "        # Shift and compress the [-1, 1] interval back to [0, 1]\n",
    "        unscaled_obs = observation\n",
    "        # Unscale the observation back to the original range\n",
    "        return unscaled_obs * (high - low) + low\n",
    "\n",
    "\n",
    "class OptimizationEnv(gym.Env):\n",
    "    def __init__(self, optimization_function, dimension, swarm_size, lower_bound, upper_bound, mix_rate=0.5,\n",
    "                     scale=False, ep_length=100, neighborhood_size=5, mutation_probability=0.2, log_scale=False,\n",
    "                     apply_mutation=True):\n",
    "        super(OptimizationEnv, self).__init__()\n",
    "        self.opt_function = optimization_function\n",
    "        self.swarm_size = swarm_size    \n",
    "        self.dimension = dimension\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.bounds = (lower_bound, upper_bound)\n",
    "        self.ep_length = ep_length\n",
    "        self.neighborhood_size = neighborhood_size\n",
    "        self.mix_rate = mix_rate    \n",
    "        self.mutation_probability = mutation_probability\n",
    "        self.scaler = ScalerHelper()\n",
    "        self.scale_option = scale\n",
    "        self.log_scale = log_scale\n",
    "        self.render_helper = Render(self)\n",
    "        self.apply_mutation = apply_mutation\n",
    "        self.fitness_cache = {}  # - Initialize the cache\n",
    "        if self.scale_option:\n",
    "            self.action_space = spaces.Box(low=-1, high=1, shape=(swarm_size, dimension), dtype=np.float32) # Continuous action space\n",
    "            self.observation_space = spaces.Box(low=0, high=1, shape=(swarm_size, dimension), dtype=np.float32)\n",
    "        else:\n",
    "            self.action_space = spaces.Box(low=-np.inf, high=np.inf, shape=(swarm_size, dimension), dtype=np.float32) # Continuous action space\n",
    "            self.observation_space = spaces.Box(low=lower_bound, high=upper_bound, shape=(swarm_size,dimension), dtype=np.float32)\n",
    "        self.reset()\n",
    "    \n",
    "    def fitness_function(self, x, *args, **kwargs):\n",
    "        # Convert x to a hashable and unique representation\n",
    "        x_hashable = tuple(map(tuple, x))\n",
    "        if x_hashable in self.fitness_cache:\n",
    "            return self.fitness_cache[x_hashable]\n",
    "        \n",
    "        if self.scale_option:\n",
    "            x = self.scaler.unscale_observation(x, self.lower_bound, self.upper_bound)\n",
    "        fitness_value = self.opt_function(x)\n",
    "        self.evaluation_counter += len(x)\n",
    "        self.fitness_cache[x_hashable] = fitness_value  # Cache the result\n",
    "        return fitness_value\n",
    "        \n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.current_step = 0\n",
    "        self.evaluation_counter = 0\n",
    "        self.state_history = np.zeros((self.swarm_size, self.ep_length+1, self.dimension+1))\n",
    "        self.gbest_history = np.zeros((self.ep_length+1, self.dimension))\n",
    "        self.state = np.random.uniform(low=self.lower_bound, high=self.upper_bound, size=(self.swarm_size, self.dimension))\n",
    "        if self.scale_option:\n",
    "            self.state = self.scaler.scale_observation(self.state, self.lower_bound, self.upper_bound)\n",
    "        self.old_state = np.copy(self.state)    \n",
    "        self.velocities = np.zeros((self.swarm_size, self.dimension))\n",
    "        self.state_history[:, self.current_step, :-1] = self.state\n",
    "        self.state_history[:, self.current_step, -1] = self.fitness_function(self.state)\n",
    "        self.pbest_positions = np.copy(self.state)\n",
    "        self.fitness_values = self.fitness_function(self.state)\n",
    "        self.gbest_position = self.state[np.argmin(self.fitness_values)]\n",
    "        self.gbest_value = np.min(self.fitness_values)\n",
    "        self.initialize_neighborhoods()\n",
    "        self.compute_lbest()\n",
    "        observation = self.generate_observation()\n",
    "        return observation\n",
    "    \n",
    "    def reward_function(self, state):\n",
    "        return -self.fitness_function(state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.compute_lbest()  # Compute Lbest positions\n",
    "        self.current_step += 1\n",
    "        self.velocities = action\n",
    "        self.old_state = np.copy(self.state)\n",
    "        self.state += action # Update the state\n",
    "        # confirm the position is within the search space\n",
    "        if self.scale_option:\n",
    "            self.state = np.clip(self.state, 0, 1)\n",
    "        else:\n",
    "            self.state = np.clip(self.state, self.lower_bound, self.upper_bound)\n",
    "        \n",
    "        self.fitness_values = self.fitness_function(self.state)\n",
    "        self.state_history[:, self.current_step, :-1] = self.state\n",
    "        self.state_history[:, self.current_step, -1] = self.fitness_values\n",
    "        \n",
    "        if self.apply_mutation:\n",
    "             # Mutation and Crossover\n",
    "            self.state = self.mutate_and_crossover(self.state, self.lbest_positions)\n",
    "            \n",
    "            self.modified_actions = self.state - self.old_state\n",
    "        else:\n",
    "            # update the personal best and global best\n",
    "            better_mask = self.fitness_values < self.fitness_function(self.pbest_positions)\n",
    "            self.pbest_positions[better_mask] = self.state[better_mask]\n",
    "            self.fitness_values[better_mask] = self.fitness_function(self.state[better_mask])\n",
    "            if np.min(self.fitness_values) < self.fitness_function(self.gbest_position.reshape(1, -1)):\n",
    "                self.gbest_position = self.state[np.argmin(self.fitness_values)]\n",
    "                self.gbest_value = np.min(self.fitness_values)\n",
    "        \n",
    "        \n",
    "        rewards = self.calculate_reward()\n",
    "        \n",
    "        if self.current_step == self.ep_length:\n",
    "            self.done = True\n",
    "            \n",
    "        observation = self.generate_observation()\n",
    "        return observation, rewards, self.done, {}\n",
    "        \n",
    "    def initialize_neighborhoods(self):\n",
    "        self.neighborhoods = []\n",
    "        for i in range(self.swarm_size):\n",
    "            neighborhood_indices = list(range(i - self.neighborhood_size, i)) + list(range(i + 1, i + 1 + self.neighborhood_size))\n",
    "            neighborhood_indices = [index % self.swarm_size for index in neighborhood_indices]  # Ensure indices are within bounds\n",
    "            self.neighborhoods.append(neighborhood_indices)\n",
    "    \n",
    "    def compute_lbest(self):\n",
    "        self.lbest_positions = np.zeros((self.swarm_size, self.dimension))\n",
    "        for i in range(self.swarm_size):\n",
    "            neighborhood_fitnesses = self.fitness_values[self.neighborhoods[i]]\n",
    "            best_neighbor_idx = self.neighborhoods[i][np.argmin(neighborhood_fitnesses)]\n",
    "            self.lbest_positions[i] = self.state[best_neighbor_idx]\n",
    "            \n",
    "    def calculate_fitness(self):\n",
    "        self.fitness_values = self.fitness_function(self.state)\n",
    "\n",
    "    def mutate(self, positions, lbest_positions):\n",
    "        # Eq. (12) from the description:\n",
    "        # Tij = Xij + A(φ × Lbest - Xij)\n",
    "        A = 2 * np.random.rand(positions.shape[0], positions.shape[1])\n",
    "        phi = np.random.rand(positions.shape[0], positions.shape[1])\n",
    "        return positions + A * (phi * lbest_positions - positions)\n",
    "    \n",
    "    def crossover(self, positions, trial_positions):\n",
    "        # For each dimension, we have a mix rate chance of taking the value from the trial position\n",
    "        crossover_mask = np.random.rand(positions.shape[0], positions.shape[1]) < self.mix_rate\n",
    "        new_positions = np.where(crossover_mask, trial_positions, positions)\n",
    "        return new_positions\n",
    "    \n",
    "    def permute_lbest(self):\n",
    "        # Apply a permutation to the lbest of all particles\n",
    "        for i in range(self.swarm_size):\n",
    "            neighbor_indices = self.neighborhoods[i]\n",
    "            np.random.shuffle(neighbor_indices)\n",
    "            self.lbest_positions[i] = self.state[neighbor_indices[0]]\n",
    "    \n",
    "    def mutate_and_crossover(self, positions, lbest_positions):\n",
    "        # Mutation and Crossove\n",
    "        #for i in range(self.swarm_size):\n",
    "        if np.random.rand() < self.mutation_probability:\n",
    "            trial_positions = self.mutate(positions, lbest_positions)\n",
    "        else:\n",
    "            self.permute_lbest()\n",
    "            trial_positions = self.mutate(positions, lbest_positions)\n",
    "\n",
    "        new_positions = self.crossover(positions, trial_positions)\n",
    "        new_fitness = self.fitness_function(new_positions)\n",
    "\n",
    "        # Selection-II: Update the personal and global bests\n",
    "        better_mask = new_fitness < self.fitness_values\n",
    "        positions[better_mask] = new_positions[better_mask]\n",
    "        self.pbest_positions[better_mask] = new_positions[better_mask]\n",
    "        self.fitness_values[better_mask] = new_fitness[better_mask]\n",
    "\n",
    "        if np.min(new_fitness) < self.fitness_function(self.gbest_position.reshape(1, -1)):\n",
    "            self.gbest_position = new_positions[np.argmin(new_fitness)]\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        # Assume gbest_value and worst_value are updated elsewhere in the environment\n",
    "        gbest_values = self.state_history[:, 0:self.current_step+1, -1].min(axis=0)\n",
    "        worst_values = self.state_history[:, 0:self.current_step+1, -1].max(axis=0)\n",
    "        \n",
    "        all_fitness_values = self.state_history[:, 0:self.current_step+1, -1]\n",
    "        scaled_fitness_values = (all_fitness_values - worst_values) / (gbest_values - worst_values)\n",
    "        \n",
    "        current_fitness = scaled_fitness_values[:, self.current_step]\n",
    "        previous_fitness = scaled_fitness_values[:, self.current_step-1]\n",
    "        \n",
    "        improvement =  current_fitness - previous_fitness\n",
    "        # Reward for improvement\n",
    "        R_pbest = np.where(improvement > 0, improvement, 0)\n",
    "        \n",
    "        R_gbest = improvement / (1 - current_fitness + 1e-3)\n",
    "\n",
    "        rewards = R_pbest + R_gbest\n",
    "        rewards = np.where(rewards < 20, rewards, 20)  \n",
    "        return rewards\n",
    "\n",
    "    def renders(self, type: str = \"state\",fps=1, file_path: Optional[str] = None, **kwargs):\n",
    "        \"\"\" Render the environment\n",
    "        Args:\n",
    "            type: type of rendering : \"state\" or \"history\"\n",
    "        \"\"\"\n",
    "        optimal_positons = kwargs.get(\"optimal_positions\", None)\n",
    "        try:\n",
    "            if type == \"state\":\n",
    "                self.render_helper.render_state(file_path, optimal_positions=optimal_positons)\n",
    "            elif type == \"history\":\n",
    "                self.render_helper.render_state_history(file_path=file_path, fps=fps, optimal_positions=optimal_positons)\n",
    "            else:\n",
    "                raise ValueError(\"type should be either 'state' or 'history'\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def get_observation(self, particle_index, dimension_index, current_step, total_steps):\n",
    "        current_velocity = self.velocities[particle_index, dimension_index]\n",
    "        personal_best_improvement = self.pbest_positions[particle_index, dimension_index] - self.state[particle_index, dimension_index]\n",
    "        local_best_improvement = self.lbest_positions[particle_index, dimension_index] - self.state[particle_index, dimension_index]\n",
    "        avg_velocity_neighborhood = np.mean([self.velocities[n_idx, dimension_index] for n_idx in self.neighborhoods[particle_index]])\n",
    "        var_velocity_neighborhood = np.var([self.velocities[n_idx, dimension_index] for n_idx in self.neighborhoods[particle_index]])\n",
    "        time_left = (total_steps - current_step) / total_steps  # Normalized time left\n",
    "        \n",
    "        observation = [\n",
    "            current_velocity, \n",
    "            personal_best_improvement, \n",
    "            local_best_improvement, \n",
    "            avg_velocity_neighborhood, \n",
    "            var_velocity_neighborhood, \n",
    "            time_left\n",
    "        ]\n",
    "        return np.array(observation)\n",
    "    \n",
    "    def generate_observation(self):\n",
    "        observations = [[self.get_observation(particle_index, dimension_index, self.current_step, self.ep_length) for dimension_index in range(self.dimension)] for particle_index in range(self.swarm_size)]\n",
    "        return np.array(observations)\n",
    "\n",
    "\n",
    "function = function_selector.get_function('f01')\n",
    "fitness_function = function['func']\n",
    "dimension = 2#function['dimension']\n",
    "lower_bound = function['domain'][0]\n",
    "upper_bound = function['domain'][1]\n",
    "global_min = function['global_min']\n",
    "scale = False\n",
    "\n",
    "acc_coefficients = (1.4, 1.4)\n",
    "inertia_weight = 0.7\n",
    "\n",
    "def get_action(env):\n",
    "    r1, r2 = np.random.rand(env.swarm_size, env.dimension), np.random.rand(env.swarm_size, env.dimension)\n",
    "    cognitive_component = acc_coefficients[0] * r1 * (env.pbest_positions - env.state)\n",
    "    social_component = acc_coefficients[1] * r2 * (env.gbest_position - env.state)\n",
    "    velocities = inertia_weight * env.velocities + cognitive_component + social_component\n",
    "    return velocities\n",
    "\n",
    "\n",
    "env = OptimizationEnv(fitness_function, dimension=dimension, lower_bound=lower_bound, \n",
    "                      upper_bound=upper_bound, swarm_size=10, scale=scale, ep_length=100, \n",
    "                      neighborhood_size=2, mutation_probability=0.2, log_scale=True, apply_mutation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "logs = []\n",
    "\n",
    "n_episodes = 2\n",
    "for i in range(n_episodes):\n",
    "    episode_log = np.empty((env.ep_length, env.swarm_size * env.dimension, 8))\n",
    "    obs = env.reset()\n",
    "    \n",
    "    episode_return = np.zeros(env.swarm_size)\n",
    "    for step in range(env.ep_length):\n",
    "        actions = get_action(env)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        # Reshape data to ensure they have the correct shape\n",
    "        obs_flat = obs.reshape(-1, 6)  # Assuming obs is already in the correct shape, just flattening\n",
    "        actions_flat = actions.reshape(-1, 1)\n",
    "        modified_actions_flat = env.modified_actions.reshape(-1, 1)\n",
    "        \n",
    "        # Concatenate observation, actions, and modified actions\n",
    "        step_data = np.concatenate((obs_flat, actions_flat, modified_actions_flat), axis=1)\n",
    "        \n",
    "        # Log the concatenated array for the current step\n",
    "        episode_log[step] = step_data\n",
    "        \n",
    "        # Check if the episode is done and break the loop if true\n",
    "        if done:\n",
    "            break\n",
    "    episode_log = episode_log.reshape(-1, 8)\n",
    "    # Append the episode log to the main logs\n",
    "    logs.append(episode_log)\n",
    "\n",
    "logs = np.array(logs).reshape(-1, 8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the log to file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(logs, columns=[\"current_velocity\", \"personal_best_improvement\", \"local_best_improvement\", \"avg_velocity_neighborhood\", \"var_velocity_neighborhood\", \"time_left\", \"action\", \"modified_action\"])\n",
    "df.to_csv(\"logs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[0][\"observations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "for i in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    episode_return = np.zeros(env.swarm_size)\n",
    "    for step in range(env.ep_length):\n",
    "        actions = get_action(env)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        # if step == env.ep_length - 1:\n",
    "        #    print(f\"Episode {i} completed - Gbest value: {env.fitness_values.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_all_dim = obs.reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_all_dim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_layers, output_dim):\n",
    "        super(DecisionTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=model_dim, nhead=4, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.input_embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        self.output_layer = nn.Linear(model_dim, output_dim)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.pos_encoder(self.input_embedding(src))\n",
    "        tgt = self.pos_encoder(self.input_embedding(tgt))\n",
    "        output = self.transformer(src, tgt)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
